"""
Saga AI Microservice
HuggingFace Spaces Ã¼zerinde Ã§alÄ±ÅŸacak semantic search + LLM servisi
"""

import os
import json
import numpy as np
from typing import List, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import faiss
from huggingface_hub import InferenceClient

# FastAPI app
app = FastAPI(
    title="Saga AI Service",
    description="Film ve kitap iÃ§in semantic search + LLM servisi",
    version="4.0.0"
)

# CORS ayarlarÄ±
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Production'da kÄ±sÄ±tla
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global deÄŸiÅŸkenler
model: SentenceTransformer = None
index: faiss.IndexFlatIP = None
content_data: List[dict] = []

# HuggingFace Inference Client (Otomatik Ã¼cretsiz provider seÃ§imi)
HF_TOKEN = os.getenv("HF_TOKEN", "")

# En iyi Ã¼cretsiz modeller (HuggingFace Inference Providers):
# - meta-llama/Llama-3.2-3B-Instruct (kÃ¼Ã§Ã¼k, hÄ±zlÄ±)
# - Qwen/Qwen2.5-7B-Instruct (orta)
# - mistralai/Mistral-7B-Instruct-v0.3 (popÃ¼ler)
# ":fastest" ekleyerek en hÄ±zlÄ± provider'Ä± seÃ§ebiliriz
LLM_MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"

# HuggingFace InferenceClient - Otomatik provider seÃ§imi yapar
hf_client: InferenceClient = None

# Pydantic modelleri
class SearchRequest(BaseModel):
    query: str
    limit: int = 5
    tur: Optional[str] = None  # film, dizi, kitap

class ContentItem(BaseModel):
    id: int
    baslik: str
    tur: str
    aciklama: str
    yil: Optional[int] = None
    posterUrl: Optional[str] = None
    puan: Optional[float] = None

class SearchResult(BaseModel):
    id: int
    baslik: str
    tur: str
    aciklama: str
    yil: Optional[int] = None
    posterUrl: Optional[str] = None
    puan: Optional[float] = None
    score: float
    neden: str  # Neden bu sonuÃ§ dÃ¶ndÃ¼

class SearchResponse(BaseModel):
    results: List[SearchResult]
    query: str
    total: int

class IndexRequest(BaseModel):
    contents: List[ContentItem]

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    index_size: int
    llm_loaded: bool


# LLM iÃ§in yeni modeller
class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 300
    temperature: float = 0.7
    system_prompt: Optional[str] = None


class GenerateResponse(BaseModel):
    text: str
    tokens_used: int


class RecommendRequest(BaseModel):
    query: str
    user_history: Optional[List[str]] = None  # KullanÄ±cÄ±nÄ±n izlediÄŸi/okuduÄŸu ÅŸeyler
    tur: Optional[str] = None
    limit: int = 5


class YearlySummaryRequest(BaseModel):
    kullanici_adi: str
    yil: int
    toplam_icerik: int
    film_sayisi: int
    dizi_sayisi: int
    kitap_sayisi: int
    toplam_dakika: int
    toplam_sayfa: int
    en_cok_izlenen_turler: List[str]
    en_yuksek_puanlilar: List[str]
    ortalama_puan: float


class YearlySummaryResponse(BaseModel):
    narrative: str
    title: str


# Film/Dizi/Kitap tanÄ±mlama iÃ§in yeni modeller
class IdentifyRequest(BaseModel):
    description: str  # KullanÄ±cÄ±nÄ±n tanÄ±mÄ±: "ellerinden penÃ§e Ã§Ä±kan adam"
    tur: Optional[str] = None  # film, dizi, kitap


class IdentifyResponse(BaseModel):
    found: bool
    title: str  # Tahmin edilen baÅŸlÄ±k: "Wolverine" veya "X-Men"
    title_en: Optional[str] = None  # Ä°ngilizce baÅŸlÄ±k (arama iÃ§in)
    tur: str  # film, dizi, kitap
    year: Optional[int] = None
    explanation: str  # Neden bu sonuÃ§: "Wolverine karakteri X-Men filmlerinde..."
    confidence: float  # GÃ¼ven skoru 0-1
    search_query: str  # TMDB/Google Books aramasÄ± iÃ§in Ã¶nerilen sorgu


# ===== YENÄ°: Genel AI Chat Modelleri =====
class ChatMessage(BaseModel):
    role: str  # "user", "assistant", "system"
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]  # Sohbet geÃ§miÅŸi
    context: Optional[str] = None  # Opsiyonel: sayfa/iÃ§erik konteksti
    max_tokens: int = 500

class ChatResponse(BaseModel):
    message: str
    suggestions: Optional[List[str]] = None  # Ã–nerilen takip sorularÄ±

class ContentQuestionRequest(BaseModel):
    content_id: Optional[int] = None
    content_title: str
    content_type: str  # film, dizi, kitap
    content_description: Optional[str] = None
    question: str  # KullanÄ±cÄ±nÄ±n sorusu

class ContentQuestionResponse(BaseModel):
    answer: str
    related_questions: Optional[List[str]] = None

class AssistantRequest(BaseModel):
    query: str  # KullanÄ±cÄ±nÄ±n sorduÄŸu ÅŸey
    current_page: Optional[str] = None  # KullanÄ±cÄ±nÄ±n bulunduÄŸu sayfa
    user_context: Optional[dict] = None  # KullanÄ±cÄ± bilgisi (izleme geÃ§miÅŸi vs.)

class AssistantResponse(BaseModel):
    message: str
    action: Optional[str] = None  # YapÄ±lacak aksiyon: "navigate", "search", "recommend", "info"
    action_data: Optional[dict] = None  # Aksiyon iÃ§in ek veri
    suggestions: Optional[List[str]] = None


def load_model():
    """Embedding modelini yÃ¼kle"""
    global model
    if model is None:
        print("ğŸ”„ Model yÃ¼kleniyor: all-MiniLM-L6-v2")
        model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… Model yÃ¼klendi!")
    return model


def load_hf_client():
    """HuggingFace InferenceClient yÃ¼kle"""
    global hf_client
    if hf_client is None:
        print(f"ğŸ”„ HuggingFace InferenceClient yÃ¼kleniyor... Model: {LLM_MODEL_NAME}")
        hf_client = InferenceClient(token=HF_TOKEN if HF_TOKEN else None)
        print("âœ… HuggingFace InferenceClient hazÄ±r!")
    return hf_client


async def call_hf_inference_api(prompt: str, max_tokens: int = 300, system_prompt: str = None) -> str:
    """HuggingFace InferenceClient ile LLM Ã§aÄŸÄ±r"""
    
    client = load_hf_client()
    
    try:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # Chat completion kullan - otomatik olarak en iyi provider seÃ§ilir
        response = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=messages,
            max_tokens=max_tokens,
            temperature=0.5
        )
        
        if response and response.choices:
            return response.choices[0].message.content
        return None
                
    except Exception as e:
        print(f"âŒ HF InferenceClient hatasÄ±: {e}")
        # Alternatif model dene
        try:
            print("ğŸ”„ Alternatif model deneniyor: microsoft/Phi-3.5-mini-instruct")
            response = client.chat.completions.create(
                model="microsoft/Phi-3.5-mini-instruct",
                messages=messages,
                max_tokens=max_tokens,
                temperature=0.5
            )
            if response and response.choices:
                return response.choices[0].message.content
        except Exception as e2:
            print(f"âŒ Alternatif model de baÅŸarÄ±sÄ±z: {e2}")
        return None


def create_search_text(item: dict) -> str:
    """Ä°Ã§erik iÃ§in aranabilir metin oluÅŸtur"""
    parts = [
        item.get('baslik', ''),
        item.get('aciklama', ''),
        item.get('tur', ''),
    ]
    if item.get('yil'):
        parts.append(str(item['yil']))
    return ' '.join(filter(None, parts))


def generate_reason(query: str, item: dict, score: float) -> str:
    """SonuÃ§ iÃ§in aÃ§Ä±klama oluÅŸtur"""
    tur_map = {
        'film': 'ğŸ¬ Film',
        'dizi': 'ğŸ“º Dizi', 
        'kitap': 'ğŸ“š Kitap'
    }
    tur_emoji = tur_map.get(item.get('tur', '').lower(), 'ğŸ­')
    
    if score > 0.7:
        return f"{tur_emoji} AradÄ±ÄŸÄ±nÄ±zla Ã§ok benzer iÃ§erik"
    elif score > 0.5:
        return f"{tur_emoji} Ä°lgili iÃ§erik bulundu"
    else:
        return f"{tur_emoji} Benzer tema"


@app.on_event("startup")
async def startup_event():
    """Uygulama baÅŸlarken modeli yÃ¼kle"""
    load_model()
    
    # EÄŸer Ã¶nceden kaydedilmiÅŸ index varsa yÃ¼kle
    if os.path.exists("faiss_index.bin") and os.path.exists("content_data.json"):
        load_index_from_disk()


def load_index_from_disk():
    """Disk'ten index ve veri yÃ¼kle"""
    global index, content_data
    try:
        index = faiss.read_index("faiss_index.bin")
        with open("content_data.json", "r", encoding="utf-8") as f:
            content_data = json.load(f)
        print(f"âœ… Index yÃ¼klendi: {len(content_data)} iÃ§erik")
    except Exception as e:
        print(f"âš ï¸ Index yÃ¼klenemedi: {e}")


def save_index_to_disk():
    """Index ve veriyi disk'e kaydet"""
    global index, content_data
    try:
        faiss.write_index(index, "faiss_index.bin")
        with open("content_data.json", "w", encoding="utf-8") as f:
            json.dump(content_data, f, ensure_ascii=False)
        print(f"âœ… Index kaydedildi: {len(content_data)} iÃ§erik")
    except Exception as e:
        print(f"âš ï¸ Index kaydedilemedi: {e}")


@app.get("/", response_model=HealthResponse)
async def health_check():
    """SaÄŸlÄ±k kontrolÃ¼"""
    return HealthResponse(
        status="healthy",
        model_loaded=model is not None,
        index_size=len(content_data),
        llm_loaded=True  # HuggingFace Inference API kullanÄ±yoruz, her zaman hazÄ±r
    )


@app.post("/index", response_model=dict)
async def index_contents(request: IndexRequest):
    """Ä°Ã§erikleri indexle (embedding oluÅŸtur)"""
    global index, content_data
    
    if not request.contents:
        raise HTTPException(status_code=400, detail="Ä°Ã§erik listesi boÅŸ")
    
    load_model()
    
    # Ä°Ã§erikleri hazÄ±rla
    content_data = [item.dict() for item in request.contents]
    texts = [create_search_text(item) for item in content_data]
    
    print(f"ğŸ”„ {len(texts)} iÃ§erik iÃ§in embedding oluÅŸturuluyor...")
    
    # Embedding oluÅŸtur
    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)
    
    # Normalize et (cosine similarity iÃ§in)
    faiss.normalize_L2(embeddings)
    
    # FAISS index oluÅŸtur
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)  # Inner Product = Cosine Similarity (normalized iÃ§in)
    index.add(embeddings)
    
    # Disk'e kaydet
    save_index_to_disk()
    
    print(f"âœ… Index oluÅŸturuldu: {index.ntotal} iÃ§erik")
    
    return {
        "success": True,
        "indexed_count": len(content_data),
        "dimension": dimension
    }


@app.post("/search", response_model=SearchResponse)
async def semantic_search(request: SearchRequest):
    """Semantic search yap"""
    global index, content_data
    
    if index is None or len(content_data) == 0:
        raise HTTPException(status_code=400, detail="Index henÃ¼z oluÅŸturulmamÄ±ÅŸ. Ã–nce /index endpoint'ini Ã§aÄŸÄ±rÄ±n.")
    
    load_model()
    
    # Query embedding
    query_embedding = model.encode([request.query], convert_to_numpy=True)
    faiss.normalize_L2(query_embedding)
    
    # Arama yap
    k = min(request.limit * 2, len(content_data))  # Filtreleme iÃ§in fazla al
    scores, indices = index.search(query_embedding, k)
    
    results = []
    for score, idx in zip(scores[0], indices[0]):
        if idx == -1:
            continue
            
        item = content_data[idx]
        
        # TÃ¼r filtresi
        if request.tur and item.get('tur', '').lower() != request.tur.lower():
            continue
        
        results.append(SearchResult(
            id=item.get('id', idx),
            baslik=item.get('baslik', ''),
            tur=item.get('tur', ''),
            aciklama=item.get('aciklama', '')[:200] + '...' if len(item.get('aciklama', '')) > 200 else item.get('aciklama', ''),
            yil=item.get('yil'),
            posterUrl=item.get('posterUrl'),
            puan=item.get('puan'),
            score=float(score),
            neden=generate_reason(request.query, item, float(score))
        ))
        
        if len(results) >= request.limit:
            break
    
    return SearchResponse(
        results=results,
        query=request.query,
        total=len(results)
    )


@app.post("/embed")
async def get_embedding(text: str):
    """Tek bir metin iÃ§in embedding dÃ¶ndÃ¼r (debug iÃ§in)"""
    load_model()
    embedding = model.encode([text], convert_to_numpy=True)
    return {"embedding": embedding[0].tolist(), "dimension": len(embedding[0])}


@app.post("/generate", response_model=GenerateResponse)
async def generate_text(request: GenerateRequest):
    """LLM ile metin Ã¼ret"""
    pipe = load_llm()
    
    if pipe is None:
        raise HTTPException(status_code=503, detail="LLM henÃ¼z yÃ¼klenmedi, lÃ¼tfen bekleyin")
    
    try:
        # Phi-3 chat format
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.prompt})
        
        result = pipe(
            messages,
            max_new_tokens=request.max_tokens,
            temperature=request.temperature,
            do_sample=True,
            pad_token_id=llm_tokenizer.eos_token_id
        )
        
        generated_text = result[0]["generated_text"]
        
        return GenerateResponse(
            text=generated_text,
            tokens_used=len(llm_tokenizer.encode(generated_text))
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ãœretim hatasÄ±: {str(e)}")


@app.post("/recommend")
async def smart_recommend(request: RecommendRequest):
    """Semantic search + LLM ile akÄ±llÄ± Ã¶neri"""
    global index, content_data
    
    if index is None or len(content_data) == 0:
        raise HTTPException(status_code=400, detail="Index henÃ¼z oluÅŸturulmamÄ±ÅŸ")
    
    load_model()
    pipe = load_llm()
    
    # Ã–nce semantic search ile benzer iÃ§erikleri bul
    query_embedding = model.encode([request.query], convert_to_numpy=True)
    faiss.normalize_L2(query_embedding)
    
    k = min(request.limit * 3, len(content_data))
    scores, indices = index.search(query_embedding, k)
    
    candidates = []
    for score, idx in zip(scores[0], indices[0]):
        if idx == -1:
            continue
        item = content_data[idx]
        if request.tur and item.get('tur', '').lower() != request.tur.lower():
            continue
        candidates.append({
            **item,
            "score": float(score)
        })
        if len(candidates) >= request.limit:
            break
    
    # LLM ile aÃ§Ä±klama ekle (opsiyonel, LLM yoksa sadece search sonucu dÃ¶ner)
    if pipe and candidates:
        for item in candidates:
            item["neden"] = f"'{request.query}' aramanÄ±za benzer iÃ§erik"
    else:
        for item in candidates:
            item["neden"] = generate_reason(request.query, item, item["score"])
    
    return {
        "query": request.query,
        "results": candidates,
        "total": len(candidates)
    }


@app.post("/yearly-summary", response_model=YearlySummaryResponse)
async def generate_yearly_summary(request: YearlySummaryRequest):
    """YÄ±llÄ±k Ã¶zet iÃ§in Spotify Wrapped tarzÄ± anlatÄ± Ã¼ret"""
    pipe = load_llm()
    
    if pipe is None:
        # LLM yoksa basit template kullan
        return YearlySummaryResponse(
            title=f"ğŸ¬ {request.kullanici_adi}'Ä±n {request.yil} YÄ±lÄ±",
            narrative=generate_fallback_narrative(request)
        )
    
    try:
        # Prompt oluÅŸtur
        prompt = f"""Sen bir medya asistanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yÄ±llÄ±k izleme/okuma istatistiklerini Spotify Wrapped tarzÄ±nda, eÄŸlenceli ve samimi bir dille anlat. TÃ¼rkÃ§e yaz.

KullanÄ±cÄ±: {request.kullanici_adi}
YÄ±l: {request.yil}

Ä°statistikler:
- Toplam iÃ§erik: {request.toplam_icerik}
- Film: {request.film_sayisi}
- Dizi: {request.dizi_sayisi}
- Kitap: {request.kitap_sayisi}
- Toplam izleme sÃ¼resi: {request.toplam_dakika} dakika ({request.toplam_dakika // 60} saat)
- Okunan sayfa: {request.toplam_sayfa}
- En sevilen tÃ¼rler: {', '.join(request.en_cok_izlenen_turler[:3])}
- En yÃ¼ksek puanlananlar: {', '.join(request.en_yuksek_puanlilar[:3])}
- Ortalama puan: {request.ortalama_puan}

KÄ±sa (3-4 cÃ¼mle), eÄŸlenceli ve kiÅŸisel bir Ã¶zet yaz. Emoji kullanabilirsin."""

        messages = [
            {"role": "system", "content": "Sen eÄŸlenceli ve samimi bir dille konuÅŸan bir medya asistanÄ±sÄ±n."},
            {"role": "user", "content": prompt}
        ]
        
        result = pipe(
            messages,
            max_new_tokens=250,
            temperature=0.8,
            do_sample=True,
            pad_token_id=llm_tokenizer.eos_token_id
        )
        
        narrative = result[0]["generated_text"]
        
        # BaÅŸlÄ±k iÃ§in kÄ±sa bir prompt
        title = f"ğŸ¬ {request.kullanici_adi}'Ä±n {request.yil} MacerasÄ±"
        
        return YearlySummaryResponse(
            title=title,
            narrative=narrative
        )
    except Exception as e:
        print(f"LLM hatasÄ±: {e}")
        return YearlySummaryResponse(
            title=f"ğŸ¬ {request.kullanici_adi}'Ä±n {request.yil} YÄ±lÄ±",
            narrative=generate_fallback_narrative(request)
        )


def generate_fallback_narrative(request: YearlySummaryRequest) -> str:
    """LLM olmadan basit anlatÄ± oluÅŸtur"""
    saat = request.toplam_dakika // 60
    
    parts = [f"Bu yÄ±l tam {request.toplam_icerik} iÃ§erik keÅŸfettin! ğŸ‰"]
    
    if request.film_sayisi > request.dizi_sayisi:
        parts.append(f"{request.film_sayisi} film ile sinema tutkunu olduÄŸun belli.")
    elif request.dizi_sayisi > 0:
        parts.append(f"{request.dizi_sayisi} dizi bitirdin, maratoncusun!")
    
    if request.kitap_sayisi > 0:
        parts.append(f"{request.toplam_sayfa} sayfa okuyarak kitap kurtlarÄ±na katÄ±ldÄ±n. ğŸ“š")
    
    if saat > 100:
        parts.append(f"{saat} saatlik izleme sÃ¼resiyle gerÃ§ek bir iÃ§erik gurmesisin!")
    
    if request.en_cok_izlenen_turler:
        parts.append(f"En sevdiÄŸin tÃ¼r: {request.en_cok_izlenen_turler[0]}.")
    
    return " ".join(parts)


# Bilinen popÃ¼ler iÃ§erikler iÃ§in pattern matching
KNOWN_CONTENT_PATTERNS = [
    # Film patterns
    {
        "patterns": ["penÃ§e", "pence", "wolverine", "adamantium", "x-men", "xmen", "logan"],
        "title": "X-Men",
        "title_en": "X-Men",
        "tur": "film",
        "year": 2000,
        "explanation": "Wolverine karakteri ellerinden Ã§Ä±kan adamantium penÃ§eleriyle tanÄ±nÄ±r"
    },
    {
        "patterns": ["yeÅŸil dev", "kÄ±zÄ±nca yeÅŸil", "hulk", "bruce banner", "gamma", "Ã¶fkelenince"],
        "title": "Hulk",
        "title_en": "The Incredible Hulk",
        "tur": "film",
        "year": 2008,
        "explanation": "Bruce Banner Ã¶fkelendiÄŸinde yeÅŸil bir deve dÃ¶nÃ¼ÅŸÃ¼r"
    },
    {
        "patterns": ["Ã¶rÃ¼mcek", "spider", "peter parker", "aÄŸ atar", "duvar tÄ±rman"],
        "title": "Ã–rÃ¼mcek Adam",
        "title_en": "Spider-Man",
        "tur": "film",
        "year": 2002,
        "explanation": "Peter Parker Ã¶rÃ¼mcek tarafÄ±ndan Ä±sÄ±rÄ±larak sÃ¼per gÃ¼Ã§ler kazanÄ±r"
    },
    {
        "patterns": ["demir adam", "iron man", "tony stark", "zÄ±rh", "arc reactor"],
        "title": "Demir Adam",
        "title_en": "Iron Man",
        "tur": "film",
        "year": 2008,
        "explanation": "Tony Stark'Ä±n yarattÄ±ÄŸÄ± teknolojik zÄ±rh"
    },
    {
        "patterns": ["yarasa", "batman", "gotham", "bruce wayne", "kara ÅŸÃ¶valye"],
        "title": "Batman",
        "title_en": "The Dark Knight",
        "tur": "film",
        "year": 2008,
        "explanation": "Bruce Wayne gece yarasa kostÃ¼mÃ¼yle suÃ§la savaÅŸÄ±r"
    },
    {
        "patterns": ["joker", "palyaÃ§o", "neden bu kadar ciddi", "why so serious"],
        "title": "Kara ÅÃ¶valye",
        "title_en": "The Dark Knight",
        "tur": "film",
        "year": 2008,
        "explanation": "Joker'in 'Why so serious?' repliÄŸiyle Ã¼nlÃ¼ film"
    },
    {
        "patterns": ["thanos", "eldiven", "parmak ÅŸÄ±klat", "infinity", "yarÄ±sÄ± yok"],
        "title": "Avengers: Sonsuzluk SavaÅŸÄ±",
        "title_en": "Avengers: Infinity War",
        "tur": "film",
        "year": 2018,
        "explanation": "Thanos Sonsuzluk Eldiveni ile evrenin yarÄ±sÄ±nÄ± yok eder"
    },
    {
        "patterns": ["matrix", "kÄ±rmÄ±zÄ± hap", "neo", "morpheus", "gerÃ§eklik simÃ¼lasyon"],
        "title": "Matrix",
        "title_en": "The Matrix",
        "tur": "film",
        "year": 1999,
        "explanation": "Neo gerÃ§ekliÄŸin bir simÃ¼lasyon olduÄŸunu keÅŸfeder"
    },
    {
        "patterns": ["yÃ¼zÃ¼k", "frodo", "mordor", "gandalf", "hobbit", "sauron", "orta dÃ¼nya"],
        "title": "YÃ¼zÃ¼klerin Efendisi",
        "title_en": "The Lord of the Rings",
        "tur": "film",
        "year": 2001,
        "explanation": "Frodo Tek YÃ¼zÃ¼k'Ã¼ yok etmek iÃ§in Mordor'a yolculuk eder"
    },
    {
        "patterns": ["hogwarts", "harry potter", "bÃ¼yÃ¼cÃ¼", "voldemort", "asasÄ±", "quidditch"],
        "title": "Harry Potter",
        "title_en": "Harry Potter",
        "tur": "film",
        "year": 2001,
        "explanation": "BÃ¼yÃ¼cÃ¼ Ã§ocuk Harry Potter'Ä±n Hogwarts maceralarÄ±"
    },
    {
        "patterns": ["titanic", "gemi batÄ±yor", "buz daÄŸÄ±", "jack", "rose", "kalbim devam"],
        "title": "Titanik",
        "title_en": "Titanic",
        "tur": "film",
        "year": 1997,
        "explanation": "Jack ve Rose'un trajik aÅŸk hikayesi"
    },
    {
        "patterns": ["inception", "rÃ¼ya iÃ§inde rÃ¼ya", "totem", "rÃ¼yaya gir"],
        "title": "BaÅŸlangÄ±Ã§",
        "title_en": "Inception",
        "tur": "film",
        "year": 2010,
        "explanation": "RÃ¼yalarÄ±n iÃ§ine girip fikir Ã§alma/yerleÅŸtirme"
    },
    {
        "patterns": ["interstellar", "kara delik", "uzay zaman", "murph", "5. boyut"],
        "title": "YÄ±ldÄ±zlararasÄ±",
        "title_en": "Interstellar",
        "tur": "film",
        "year": 2014,
        "explanation": "Ä°nsanlÄ±ÄŸÄ± kurtarmak iÃ§in uzay yolculuÄŸu"
    },
    {
        "patterns": ["fight club", "dÃ¶vÃ¼ÅŸ kulÃ¼bÃ¼", "tyler durden", "ilk kural konuÅŸma"],
        "title": "DÃ¶vÃ¼ÅŸ KulÃ¼bÃ¼",
        "title_en": "Fight Club",
        "tur": "film",
        "year": 1999,
        "explanation": "Tyler Durden'Ä±n kurduÄŸu yeraltÄ± dÃ¶vÃ¼ÅŸ kulÃ¼bÃ¼"
    },
    {
        "patterns": ["forrest gump", "koÅŸ forrest", "Ã§ikolata kutusu", "hayat kutu"],
        "title": "Forrest Gump",
        "title_en": "Forrest Gump",
        "tur": "film",
        "year": 1994,
        "explanation": "Forrest Gump'Ä±n olaÄŸanÃ¼stÃ¼ hayat hikayesi"
    },
    # Dizi patterns
    {
        "patterns": ["breaking bad", "heisenberg", "meth", "walter white", "kimya Ã¶ÄŸretmen", "kimya ogretmen", "uyuÅŸturucu Ã¼ret", "uyusturucu uret", "blue meth"],
        "title": "Breaking Bad",
        "title_en": "Breaking Bad",
        "tur": "dizi",
        "year": 2008,
        "explanation": "Kimya Ã¶ÄŸretmeni uyuÅŸturucu imparatorluÄŸu kurar"
    },
    {
        "patterns": ["game of thrones", "taht oyunlarÄ±", "westeros", "ejderha", "kÄ±ÅŸ geliyor"],
        "title": "Taht OyunlarÄ±",
        "title_en": "Game of Thrones",
        "tur": "dizi",
        "year": 2011,
        "explanation": "Demir Taht iÃ§in krallÄ±klar savaÅŸÄ±"
    },
    {
        "patterns": ["stranger things", "demogorgon", "upside down", "eleven", "80ler"],
        "title": "Stranger Things",
        "title_en": "Stranger Things",
        "tur": "dizi",
        "year": 2016,
        "explanation": "Hawkins kasabasÄ±nÄ±n paranormal olaylarÄ±"
    },
    {
        "patterns": ["friends", "central perk", "ross rachel", "how you doin"],
        "title": "Friends",
        "title_en": "Friends",
        "tur": "dizi",
        "year": 1994,
        "explanation": "New York'ta 6 arkadaÅŸÄ±n hikayesi"
    },
    {
        "patterns": ["office", "michael scott", "dunder mifflin", "that's what she said"],
        "title": "Ofis",
        "title_en": "The Office",
        "tur": "dizi",
        "year": 2005,
        "explanation": "KaÄŸÄ±t ÅŸirketindeki ofis Ã§alÄ±ÅŸanlarÄ±nÄ±n hayatÄ±"
    },
    {
        "patterns": ["money heist", "la casa de papel", "professor", "bella ciao", "soygun"],
        "title": "La Casa de Papel",
        "title_en": "Money Heist",
        "tur": "dizi",
        "year": 2017,
        "explanation": "Darphane soygunu planÄ±"
    },
    # Kitap patterns
    {
        "patterns": ["1984", "bÃ¼yÃ¼k birader", "big brother", "orwell"],
        "title": "1984",
        "title_en": "1984",
        "tur": "kitap",
        "year": 1949,
        "explanation": "George Orwell'Ä±n distopik romanÄ±"
    },
    {
        "patterns": ["suÃ§ ve ceza", "raskolnikov", "dostoyevski", "cinayet vicdan"],
        "title": "SuÃ§ ve Ceza",
        "title_en": "Crime and Punishment",
        "tur": "kitap",
        "year": 1866,
        "explanation": "Raskolnikov'un cinayet ve vicdan azabÄ± hikayesi"
    },
]


def match_known_content(description: str) -> Optional[IdentifyResponse]:
    """Bilinen iÃ§erik kalÄ±plarÄ±nÄ± eÅŸleÅŸtir"""
    desc_lower = description.lower()
    
    for content in KNOWN_CONTENT_PATTERNS:
        for pattern in content["patterns"]:
            if pattern.lower() in desc_lower:
                return IdentifyResponse(
                    found=True,
                    title=content["title"],
                    title_en=content["title_en"],
                    tur=content["tur"],
                    year=content["year"],
                    explanation=content["explanation"],
                    confidence=0.95,
                    search_query=content["title_en"]
                )
    
    return None


@app.post("/identify", response_model=IdentifyResponse)
async def identify_content(request: IdentifyRequest):
    """
    KullanÄ±cÄ±nÄ±n tanÄ±mÄ±ndan film/dizi/kitap adÄ±nÄ± tahmin et.
    Ã–rnek: "ellerinden penÃ§e Ã§Ä±kan adam" -> "Wolverine / X-Men"
    """
    
    # Ã–nce bilinen popÃ¼ler iÃ§erikler iÃ§in pattern matching dene
    known_content = match_known_content(request.description)
    if known_content:
        return known_content
    
    # HuggingFace Inference API ile LLM Ã§aÄŸÄ±r
    try:
        tur_hint = ""
        if request.tur:
            tur_map = {"film": "film", "dizi": "TV dizisi", "kitap": "kitap"}
            tur_hint = f"Bu bir {tur_map.get(request.tur, request.tur)} olmalÄ±."
        
        # Kimi-K2 iÃ§in OpenAI uyumlu format
        system_prompt = "Sen bir film, dizi ve kitap uzmanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n verdiÄŸi tanÄ±mdan hangi iÃ§erik olduÄŸunu bul. SADECE JSON formatÄ±nda cevap ver, baÅŸka hiÃ§bir ÅŸey yazma."
        
        user_prompt = f"""KullanÄ±cÄ±nÄ±n tanÄ±mÄ±: "{request.description}"
{tur_hint}

AÅŸaÄŸÄ±daki formatta JSON olarak cevap ver:
{{
    "title": "Ä°Ã§eriÄŸin TÃ¼rkÃ§e adÄ±",
    "title_en": "Ä°Ã§eriÄŸin Ä°ngilizce adÄ±",
    "tur": "film/dizi/kitap",
    "year": yÄ±l (bilinmiyorsa null),
    "explanation": "Neden bu iÃ§erik olduÄŸunu kÄ±sa aÃ§Ä±kla",
    "confidence": 0.0-1.0 arasÄ± gÃ¼ven skoru
}}

Ã–rnek: "ellerinden penÃ§e Ã§Ä±kan adam" iÃ§in:
{{"title": "X-Men", "title_en": "X-Men", "tur": "film", "year": 2000, "explanation": "Wolverine karakteri adamantium penÃ§eleriyle tanÄ±nÄ±r", "confidence": 0.95}}"""

        response_text = await call_hf_inference_api(user_prompt, max_tokens=250, system_prompt=system_prompt)
        
        if not response_text:
            # API Ã§alÄ±ÅŸmadÄ±, pattern matching sonucunu dÃ¶ndÃ¼r
            return IdentifyResponse(
                found=False,
                title="",
                title_en=None,
                tur=request.tur or "film",
                year=None,
                explanation="LLM API'ye ulaÅŸÄ±lamadÄ±",
                confidence=0.0,
                search_query=request.description
            )
        
        print(f"LLM yanÄ±tÄ±: {response_text}")
        
        # JSON parse et
        import re
        json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
        
        if json_match:
            parsed = json.loads(json_match.group())
            
            title = parsed.get("title", "")
            title_en = parsed.get("title_en", title)
            tur = parsed.get("tur", "film")
            
            # Arama sorgusu oluÅŸtur
            search_query = title_en if title_en else title
            
            return IdentifyResponse(
                found=True,
                title=title,
                title_en=title_en,
                tur=tur,
                year=parsed.get("year"),
                explanation=parsed.get("explanation", ""),
                confidence=float(parsed.get("confidence", 0.5)),
                search_query=search_query
            )
        else:
            # JSON bulunamadÄ±
            return IdentifyResponse(
                found=False,
                title="",
                title_en=None,
                tur=request.tur or "film",
                year=None,
                explanation="TanÄ±mdan iÃ§erik belirlenemedi",
                confidence=0.0,
                search_query=request.description
            )
            
    except json.JSONDecodeError as e:
        print(f"JSON parse hatasÄ±: {e}")
        return IdentifyResponse(
            found=False,
            title="",
            title_en=None,
            tur=request.tur or "film",
            year=None,
            explanation="YanÄ±t iÅŸlenemedi",
            confidence=0.0,
            search_query=request.description
        )
    except Exception as e:
        print(f"Identify hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=f"Ä°Ã§erik tanÄ±mlama hatasÄ±: {str(e)}")


# ===== YENÄ°: Genel AI Chat Endpoint =====
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Genel sohbet endpoint'i - KullanÄ±cÄ±yla doÄŸal dilde konuÅŸma
    Film, dizi, kitap hakkÄ±nda her tÃ¼rlÃ¼ soruyu yanÄ±tlar
    """
    try:
        # Saga asistanÄ± system prompt'u
        system_prompt = """Sen Saga'nÄ±n AI asistanÄ±sÄ±n. Saga, kullanÄ±cÄ±larÄ±n film, dizi ve kitaplarÄ± takip ettiÄŸi bir platformdur.

GÃ¶revlerin:
1. Film, dizi ve kitaplar hakkÄ±nda bilgi vermek (Ã¶zet, oyuncular, yÃ¶netmenler, yazarlar, tÃ¼rler vs.)
2. Ä°Ã§erik Ã¶nerileri yapmak
3. KullanÄ±cÄ±nÄ±n sorularÄ±nÄ± yanÄ±tlamak
4. Platform hakkÄ±nda yardÄ±m etmek

Kurallar:
- TÃ¼rkÃ§e yanÄ±t ver
- KÄ±sa ve Ã¶z ol, gereksiz uzatma
- Spoiler vermekten kaÃ§Ä±n (kullanÄ±cÄ± aÃ§Ä±kÃ§a istemezse)
- Emin olmadÄ±ÄŸÄ±n bilgileri tahmin olarak belirt
- Samimi ve yardÄ±msever ol"""

        # MesajlarÄ± OpenAI formatÄ±na Ã§evir
        messages = [{"role": "system", "content": system_prompt}]
        
        # Kontekst varsa ekle
        if request.context:
            messages.append({"role": "system", "content": f"KullanÄ±cÄ± ÅŸu anda ÅŸu sayfada: {request.context}"})
        
        # Sohbet geÃ§miÅŸini ekle
        for msg in request.messages:
            messages.append({"role": msg.role, "content": msg.content})
        
        client = load_hf_client()
        if not client:
            return ChatResponse(
                message="AI servisi ÅŸu anda kullanÄ±lamÄ±yor. LÃ¼tfen daha sonra tekrar deneyin.",
                suggestions=None
            )
        
        try:
            completion = client.chat.completions.create(
                model=LLM_MODEL_NAME,
                messages=messages,
                max_tokens=request.max_tokens,
                temperature=0.7
            )
            
            response_text = completion.choices[0].message.content
        except Exception as api_err:
            print(f"Chat API hatasÄ±: {api_err}")
            return ChatResponse(
                message="AI yanÄ±t veremedi. LÃ¼tfen tekrar deneyin.",
                suggestions=None
            )
        
        # Takip sorularÄ± Ã¶ner
        suggestions = None
        if len(request.messages) <= 2:  # Ä°lk birkaÃ§ mesajda Ã¶neri ver
            suggestions = [
                "Bu iÃ§eriÄŸe benzer baÅŸka Ã¶nerilerin var mÄ±?",
                "OyuncularÄ±/yazarÄ± hakkÄ±nda bilgi verir misin?",
                "Bu iÃ§eriÄŸin puanÄ± nasÄ±l?"
            ]
        
        return ChatResponse(
            message=response_text,
            suggestions=suggestions
        )
        
    except Exception as e:
        print(f"Chat hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=f"Sohbet hatasÄ±: {str(e)}")


# ===== YENÄ°: Ä°Ã§erik HakkÄ±nda Soru-Cevap =====
@app.post("/content-question", response_model=ContentQuestionResponse)
async def content_question(request: ContentQuestionRequest):
    """
    Belirli bir iÃ§erik hakkÄ±nda soru yanÄ±tla
    Ã–rnek: "Inception filminin konusu ne?" veya "Bu kitabÄ±n yazarÄ± kim?"
    """
    try:
        system_prompt = f"""Sen bir {request.content_type} uzmanÄ±sÄ±n. KullanÄ±cÄ± "{request.content_title}" hakkÄ±nda soru soruyor.

Ä°Ã§erik bilgisi:
- BaÅŸlÄ±k: {request.content_title}
- TÃ¼r: {request.content_type}
{f'- AÃ§Ä±klama: {request.content_description}' if request.content_description else ''}

Kurallar:
- TÃ¼rkÃ§e yanÄ±t ver
- Spoiler vermekten kaÃ§Ä±n (aÃ§Ä±kÃ§a istenmezse)
- KÄ±sa ve bilgilendirici ol
- Emin olmadÄ±ÄŸÄ±n bilgileri belirt"""

        user_prompt = request.question
        
        client = load_hf_client()
        if not client:
            return ContentQuestionResponse(
                answer="AI servisi ÅŸu anda kullanÄ±lamÄ±yor.",
                related_questions=None
            )
        
        try:
            completion = client.chat.completions.create(
                model=LLM_MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=400,
                temperature=0.6
            )
            
            answer = completion.choices[0].message.content
        except Exception as api_err:
            print(f"Content question API hatasÄ±: {api_err}")
            return ContentQuestionResponse(
                answer="AI yanÄ±t veremedi. LÃ¼tfen tekrar deneyin.",
                related_questions=None
            )
        
        # Ä°lgili sorular Ã¶ner
        related_questions = [
            f"{request.content_title} ile benzer iÃ§erikler neler?",
            f"Bu {request.content_type}Ä±n puanÄ± kaÃ§?",
            f"KÄ±saca Ã¶zet verir misin?"
        ]
        
        return ContentQuestionResponse(
            answer=answer,
            related_questions=related_questions
        )
        
    except Exception as e:
        print(f"Content question hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=f"Soru yanÄ±tlama hatasÄ±: {str(e)}")


# ===== YENÄ°: Site AsistanÄ± =====
@app.post("/assistant", response_model=AssistantResponse)
async def assistant(request: AssistantRequest):
    """
    Site genelinde akÄ±llÄ± asistan
    Navigasyon, arama, Ã¶neri ve bilgi saÄŸlar
    """
    try:
        system_prompt = """Sen Saga platformunun akÄ±llÄ± asistanÄ±sÄ±n. KullanÄ±cÄ±lara yardÄ±m ediyorsun.

Platform Ã¶zellikleri:
- Film, dizi ve kitap takibi
- KÃ¼tÃ¼phane: Ä°zlenenler, okunanlar, izlenecekler
- Listeler: Ã–zel koleksiyonlar oluÅŸturma
- KeÅŸfet: Yeni iÃ§erik bulma
- Profil: KullanÄ±cÄ± istatistikleri
- YÄ±llÄ±k Ã¶zet: YÄ±l sonu deÄŸerlendirmesi

YapabileceÄŸin aksiyonlar:
1. "navigate" - KullanÄ±cÄ±yÄ± bir sayfaya yÃ¶nlendir
2. "search" - Ä°Ã§erik aramasÄ± yap
3. "recommend" - Ã–neri yap
4. "info" - Bilgi ver

Her yanÄ±tta:
1. KullanÄ±cÄ±nÄ±n isteÄŸini anla
2. Uygun aksiyonu belirle
3. KÄ±sa ve yardÄ±msever yanÄ±t ver

JSON formatÄ±nda yanÄ±t ver:
{
    "message": "KullanÄ±cÄ±ya mesaj",
    "action": "navigate/search/recommend/info veya null",
    "action_data": {"url": "/sayfa", "query": "arama", "items": [...]} veya null,
    "suggestions": ["Ã–neri 1", "Ã–neri 2"]
}"""

        user_context_str = ""
        if request.user_context:
            user_context_str = f"\nKullanÄ±cÄ± bilgisi: {json.dumps(request.user_context, ensure_ascii=False)}"
        
        page_context = ""
        if request.current_page:
            page_context = f"\nKullanÄ±cÄ± ÅŸu anda '{request.current_page}' sayfasÄ±nda."
        
        user_prompt = f"""{request.query}{page_context}{user_context_str}

JSON formatÄ±nda yanÄ±t ver."""

        client = load_hf_client()
        if not client:
            return AssistantResponse(
                message="AI asistan ÅŸu anda kullanÄ±lamÄ±yor. Arama yapabilir veya menÃ¼yÃ¼ kullanabilirsiniz.",
                action=None,
                action_data=None,
                suggestions=["KeÅŸfet sayfasÄ±na git", "KÃ¼tÃ¼phaneme bak", "Arama yap"]
            )
        
        try:
            completion = client.chat.completions.create(
                model=LLM_MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=400,
                temperature=0.5
            )
            
            response_text = completion.choices[0].message.content
        except Exception as api_err:
            print(f"Assistant API hatasÄ±: {api_err}")
            return AssistantResponse(
                message="AI asistan yanÄ±t veremedi. LÃ¼tfen tekrar deneyin.",
                action=None,
                action_data=None,
                suggestions=["KeÅŸfet sayfasÄ±na git", "KÃ¼tÃ¼phaneme bak"]
            )
        
        # JSON parse etmeye Ã§alÄ±ÅŸ
        import re
        json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
        
        if json_match:
            try:
                parsed = json.loads(json_match.group())
                return AssistantResponse(
                    message=parsed.get("message", response_text),
                    action=parsed.get("action"),
                    action_data=parsed.get("action_data"),
                    suggestions=parsed.get("suggestions")
                )
            except:
                pass
        
        # JSON parse edilemezse dÃ¼z metin olarak dÃ¶ndÃ¼r
        return AssistantResponse(
            message=response_text,
            action=None,
            action_data=None,
            suggestions=["BaÅŸka bir ÅŸey sormak ister misin?"]
        )
        
    except Exception as e:
        print(f"Assistant hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=f"Asistan hatasÄ±: {str(e)}")


# ===== YENÄ°: Ã–zet Ä°ste =====
@app.post("/summarize")
async def summarize_content(content_title: str, content_type: str, spoiler_free: bool = True):
    """
    Bir iÃ§eriÄŸin Ã¶zetini al
    """
    try:
        spoiler_note = "SPOILER VERME!" if spoiler_free else "Spoiler verebilirsin."
        
        system_prompt = f"""Sen bir {content_type} uzmanÄ±sÄ±n. "{content_title}" iÃ§in kÄ±sa bir Ã¶zet yaz.
{spoiler_note}
TÃ¼rkÃ§e yaz. 2-3 paragraf yeterli."""

        client = load_hf_client()
        if not client:
            return {"summary": "AI servisi kullanÄ±lamÄ±yor.", "spoiler_free": spoiler_free}
        
        try:
            completion = client.chat.completions.create(
                model=LLM_MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"{content_title} hakkÄ±nda Ã¶zet ver."}
                ],
                max_tokens=500,
                temperature=0.6
            )
            
            return {
                "title": content_title,
                "type": content_type,
                "summary": completion.choices[0].message.content,
                "spoiler_free": spoiler_free
            }
        except Exception as e:
            print(f"Summarize API hatasÄ±: {e}")
            return {"summary": f"Ã–zet alÄ±namadÄ±: {str(e)}", "spoiler_free": spoiler_free}
        
    except Exception as e:
        print(f"Summarize hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=f"Ã–zet hatasÄ±: {str(e)}")


# Gradio interface (HuggingFace Spaces iÃ§in)
def create_gradio_interface():
    """Gradio arayÃ¼zÃ¼ oluÅŸtur (opsiyonel)"""
    try:
        import gradio as gr
        
        def search_ui(query: str, tur: str, limit: int):
            if index is None:
                return "Index yok. Ã–nce iÃ§erikleri yÃ¼kleyin."
            
            tur_filter = tur if tur != "Hepsi" else None
            
            query_embedding = model.encode([query], convert_to_numpy=True)
            faiss.normalize_L2(query_embedding)
            
            scores, indices = index.search(query_embedding, limit)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx == -1:
                    continue
                item = content_data[idx]
                if tur_filter and item.get('tur', '').lower() != tur_filter.lower():
                    continue
                results.append(f"**{item.get('baslik')}** ({item.get('tur')}) - Skor: {score:.2f}\n{item.get('aciklama', '')[:100]}...")
            
            return "\n\n---\n\n".join(results) if results else "SonuÃ§ bulunamadÄ±"
        
        interface = gr.Interface(
            fn=search_ui,
            inputs=[
                gr.Textbox(label="Arama", placeholder="Film/kitap anlat veya ara..."),
                gr.Dropdown(["Hepsi", "film", "dizi", "kitap"], label="TÃ¼r", value="Hepsi"),
                gr.Slider(1, 10, value=5, step=1, label="SonuÃ§ SayÄ±sÄ±")
            ],
            outputs=gr.Markdown(label="SonuÃ§lar"),
            title="ğŸ¬ Saga AI Search",
            description="Film, dizi ve kitap iÃ§in semantic arama"
        )
        
        return interface
    except ImportError:
        return None


# HuggingFace Spaces iÃ§in
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=7860)
