"""
Saga AI Microservice
HuggingFace Spaces Ã¼zerinde Ã§alÄ±ÅŸacak semantic search + LLM servisi
"""

import os
import json
import numpy as np
import httpx
from typing import List, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

# FastAPI app
app = FastAPI(
    title="Saga AI Service",
    description="Film ve kitap iÃ§in semantic search + LLM servisi",
    version="5.0.0"
)

# CORS ayarlarÄ±
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Production'da kÄ±sÄ±tla
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global deÄŸiÅŸkenler
model: SentenceTransformer = None
index: faiss.IndexFlatIP = None
content_data: List[dict] = []

# Groq API - Ãœcretsiz, Ã§ok hÄ±zlÄ±, Ã§ok akÄ±llÄ±!
# llama-3.3-70b-versatile: 30 req/min, 1K req/day, 12K tokens/min
GROQ_API_KEY = os.environ.get("GROQ_API_KEY", "")
GROQ_MODEL = "llama-3.3-70b-versatile"  # En akÄ±llÄ± model!
USE_GROQ = bool(GROQ_API_KEY)

# Lokal model (Groq yoksa fallback)
LLM_MODEL_NAME = "Qwen/Qwen2.5-3B-Instruct"
llm_tokenizer = None
llm_model = None
llm_pipe = None

# Pydantic modelleri
class SearchRequest(BaseModel):
    query: str
    limit: int = 5
    tur: Optional[str] = None  # film, dizi, kitap

class ContentItem(BaseModel):
    id: int
    baslik: str
    tur: str
    aciklama: str
    yil: Optional[int] = None
    posterUrl: Optional[str] = None
    puan: Optional[float] = None

class SearchResult(BaseModel):
    id: int
    baslik: str
    tur: str
    aciklama: str
    yil: Optional[int] = None
    posterUrl: Optional[str] = None
    puan: Optional[float] = None
    score: float
    neden: str  # Neden bu sonuÃ§ dÃ¶ndÃ¼

class SearchResponse(BaseModel):
    results: List[SearchResult]
    query: str
    total: int

class IndexRequest(BaseModel):
    contents: List[ContentItem]

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    index_size: int
    llm_loaded: bool


# LLM iÃ§in yeni modeller
class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 300
    temperature: float = 0.7
    system_prompt: Optional[str] = None


class GenerateResponse(BaseModel):
    text: str
    tokens_used: int


class RecommendRequest(BaseModel):
    query: str
    user_history: Optional[List[str]] = None  # KullanÄ±cÄ±nÄ±n izlediÄŸi/okuduÄŸu ÅŸeyler
    tur: Optional[str] = None
    limit: int = 5


class YearlySummaryRequest(BaseModel):
    kullanici_adi: str
    yil: int
    toplam_icerik: int
    film_sayisi: int
    dizi_sayisi: int
    kitap_sayisi: int
    toplam_dakika: int
    toplam_sayfa: int
    en_cok_izlenen_turler: List[str]
    en_yuksek_puanlilar: List[str]
    ortalama_puan: float


class YearlySummaryResponse(BaseModel):
    narrative: str
    title: str


# Film/Dizi/Kitap tanÄ±mlama iÃ§in yeni modeller
class IdentifyRequest(BaseModel):
    description: str  # KullanÄ±cÄ±nÄ±n tanÄ±mÄ±: "ellerinden penÃ§e Ã§Ä±kan adam"
    tur: Optional[str] = None  # film, dizi, kitap


class IdentifyResponse(BaseModel):
    found: bool
    title: str  # Tahmin edilen baÅŸlÄ±k: "Wolverine" veya "X-Men"
    title_en: Optional[str] = None  # Ä°ngilizce baÅŸlÄ±k (arama iÃ§in)
    tur: str  # film, dizi, kitap
    year: Optional[int] = None
    explanation: str  # Neden bu sonuÃ§: "Wolverine karakteri X-Men filmlerinde..."
    confidence: float  # GÃ¼ven skoru 0-1
    search_query: str  # TMDB/Google Books aramasÄ± iÃ§in Ã¶nerilen sorgu


# ===== YENÄ°: Genel AI Chat Modelleri =====
class ChatMessage(BaseModel):
    role: str  # "user", "assistant", "system"
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]  # Sohbet geÃ§miÅŸi
    context: Optional[str] = None  # Opsiyonel: sayfa/iÃ§erik konteksti
    max_tokens: int = 500

class ChatResponse(BaseModel):
    message: str
    suggestions: Optional[List[str]] = None  # Ã–nerilen takip sorularÄ±

class ContentQuestionRequest(BaseModel):
    content_id: Optional[int] = None
    content_title: str
    content_type: str  # film, dizi, kitap
    content_description: Optional[str] = None
    question: str  # KullanÄ±cÄ±nÄ±n sorusu

class ContentQuestionResponse(BaseModel):
    answer: str
    related_questions: Optional[List[str]] = None

class ChatHistoryItem(BaseModel):
    role: str  # "user" veya "assistant"
    content: str

class AssistantRequest(BaseModel):
    query: str  # KullanÄ±cÄ±nÄ±n sorduÄŸu ÅŸey
    current_page: Optional[str] = None  # KullanÄ±cÄ±nÄ±n bulunduÄŸu sayfa
    user_context: Optional[dict] = None  # KullanÄ±cÄ± bilgisi (izleme geÃ§miÅŸi vs.)
    chat_history: Optional[List[ChatHistoryItem]] = None  # Ã–nceki sohbet mesajlarÄ±

class AssistantResponse(BaseModel):
    message: str
    action: Optional[str] = None  # YapÄ±lacak aksiyon: "navigate", "search", "recommend", "info"
    action_data: Optional[dict] = None  # Aksiyon iÃ§in ek veri
    suggestions: Optional[List[str]] = None


def load_model():
    """Embedding modelini yÃ¼kle"""
    global model
    if model is None:
        print("ğŸ”„ Model yÃ¼kleniyor: all-MiniLM-L6-v2")
        model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… Model yÃ¼klendi!")
    return model


def load_llm():
    """Lokal LLM modelini yÃ¼kle"""
    global llm_tokenizer, llm_model, llm_pipe
    
    if USE_GROQ:
        print("âœ… Groq API kullanÄ±lacak (Llama 3.1 70B)")
        return True
    
    if llm_pipe is None:
        print(f"ğŸ”„ LLM yÃ¼kleniyor: {LLM_MODEL_NAME}")
        try:
            llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)
            llm_model = AutoModelForCausalLM.from_pretrained(
                LLM_MODEL_NAME,
                torch_dtype=torch.float32,
                device_map="cpu",
                low_cpu_mem_usage=True
            )
            llm_pipe = pipeline(
                "text-generation",
                model=llm_model,
                tokenizer=llm_tokenizer,
                device="cpu"
            )
            print("âœ… LLM yÃ¼klendi!")
        except Exception as e:
            print(f"âŒ LLM yÃ¼klenemedi: {e}")
            llm_pipe = None
    
    return llm_pipe


async def call_groq_api(messages: list, max_tokens: int = 300) -> str:
    """Groq API ile yanÄ±t Ã¼ret - Ã‡OK HIZLI!"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                "https://api.groq.com/openai/v1/chat/completions",
                json={
                    "model": GROQ_MODEL,
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": 0.7
                },
                headers={
                    "Authorization": f"Bearer {GROQ_API_KEY}",
                    "Content-Type": "application/json"
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                print(f"âŒ Groq API hatasÄ±: {response.status_code} - {response.text}")
                return None
    except Exception as e:
        print(f"âŒ Groq API Ã§aÄŸrÄ± hatasÄ±: {e}")
        return None


async def call_local_llm(messages: list, max_tokens: int = 300) -> str:
    """LLM ile yanÄ±t Ã¼ret - Groq varsa onu kullan, yoksa lokal"""
    
    if USE_GROQ:
        return await call_groq_api(messages, max_tokens)
    
    pipe = load_llm()
    if pipe is None:
        return None
    
    try:
        response = pipe(
            messages,
            max_new_tokens=max_tokens,
            temperature=0.7,
            do_sample=True,
            pad_token_id=llm_tokenizer.eos_token_id
        )
        
        if response and len(response) > 0:
            generated = response[0].get("generated_text", [])
            if isinstance(generated, list) and len(generated) > 0:
                for msg in reversed(generated):
                    if msg.get("role") == "assistant":
                        return msg.get("content", "")
            elif isinstance(generated, str):
                return generated
        
        return None
        
    except Exception as e:
        print(f"âŒ LLM Ã§aÄŸrÄ± hatasÄ±: {e}")
        return None


# Geriye uyumluluk iÃ§in eski fonksiyon adlarÄ±
async def call_hf_serverless_api(messages: list, max_tokens: int = 300) -> str:
    return await call_local_llm(messages, max_tokens)

async def call_hf_router_api(messages: list, max_tokens: int = 300) -> str:
    return await call_local_llm(messages, max_tokens)


async def call_hf_inference_api(prompt: str, max_tokens: int = 300, system_prompt: str = None) -> str:
    """LLM Ã§aÄŸÄ±r"""
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    return await call_local_llm(messages, max_tokens)


def extract_json(text: str) -> Optional[str]:
    """Metinden JSON objesini Ã§Ä±kar (iÃ§ iÃ§e {} destekli)"""
    import re
    
    if not text:
        return None
    
    # Ã–nce ```json bloÄŸu ara
    code_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
    if code_match:
        return code_match.group(1)
    
    # Sonra normal JSON bloÄŸu ara - brace counting ile
    start = text.find('{')
    if start == -1:
        return None
    
    depth = 0
    for i, char in enumerate(text[start:], start):
        if char == '{':
            depth += 1
        elif char == '}':
            depth -= 1
            if depth == 0:
                return text[start:i+1]
    return None


def parse_assistant_response(text: str) -> dict:
    """Asistan yanÄ±tÄ±nÄ± parse et - JSON veya dÃ¼z metin"""
    if not text:
        return {"message": "YanÄ±t alÄ±namadÄ±", "action": None, "action_data": None, "suggestions": None}
    
    # JSON parse dene
    json_str = extract_json(text)
    if json_str:
        try:
            parsed = json.loads(json_str)
            # message iÃ§inde de JSON string varsa tekrar parse et
            if isinstance(parsed.get("message"), str) and parsed["message"].strip().startswith("{"):
                inner_json = extract_json(parsed["message"])
                if inner_json:
                    try:
                        inner_parsed = json.loads(inner_json)
                        # Ä°Ã§ JSON'Ä± ana yanÄ±tla birleÅŸtir
                        return {
                            "message": inner_parsed.get("message", parsed["message"]),
                            "action": inner_parsed.get("action") or parsed.get("action"),
                            "action_data": inner_parsed.get("action_data") or parsed.get("action_data"),
                            "suggestions": inner_parsed.get("suggestions") or parsed.get("suggestions")
                        }
                    except:
                        pass
            return parsed
        except json.JSONDecodeError:
            pass
    
    # JSON yoksa dÃ¼z metin olarak iÅŸle
    return {"message": text.strip(), "action": None, "action_data": None, "suggestions": None}


def create_search_text(item: dict) -> str:
    """Ä°Ã§erik iÃ§in aranabilir metin oluÅŸtur"""
    parts = [
        item.get('baslik', ''),
        item.get('aciklama', ''),
        item.get('tur', ''),
    ]
    if item.get('yil'):
        parts.append(str(item['yil']))
    return ' '.join(filter(None, parts))


def generate_reason(query: str, item: dict, score: float) -> str:
    """SonuÃ§ iÃ§in aÃ§Ä±klama oluÅŸtur"""
    tur_map = {
        'film': 'ğŸ¬ Film',
        'dizi': 'ğŸ“º Dizi', 
        'kitap': 'ğŸ“š Kitap'
    }
    tur_emoji = tur_map.get(item.get('tur', '').lower(), 'ğŸ­')
    
    if score > 0.7:
        return f"{tur_emoji} AradÄ±ÄŸÄ±nÄ±zla Ã§ok benzer iÃ§erik"
    elif score > 0.5:
        return f"{tur_emoji} Ä°lgili iÃ§erik bulundu"
    else:
        return f"{tur_emoji} Benzer tema"


@app.on_event("startup")
async def startup_event():
    """Uygulama baÅŸlarken modeli yÃ¼kle"""
    load_model()
    
    # EÄŸer Ã¶nceden kaydedilmiÅŸ index varsa yÃ¼kle
    if os.path.exists("faiss_index.bin") and os.path.exists("content_data.json"):
        load_index_from_disk()


def load_index_from_disk():
    """Disk'ten index ve veri yÃ¼kle"""
    global index, content_data
    try:
        index = faiss.read_index("faiss_index.bin")
        with open("content_data.json", "r", encoding="utf-8") as f:
            content_data = json.load(f)
        print(f"âœ… Index yÃ¼klendi: {len(content_data)} iÃ§erik")
    except Exception as e:
        print(f"âš ï¸ Index yÃ¼klenemedi: {e}")


def save_index_to_disk():
    """Index ve veriyi disk'e kaydet"""
    global index, content_data
    try:
        faiss.write_index(index, "faiss_index.bin")
        with open("content_data.json", "w", encoding="utf-8") as f:
            json.dump(content_data, f, ensure_ascii=False)
        print(f"âœ… Index kaydedildi: {len(content_data)} iÃ§erik")
    except Exception as e:
        print(f"âš ï¸ Index kaydedilemedi: {e}")


@app.get("/", response_model=HealthResponse)
async def health_check():
    """SaÄŸlÄ±k kontrolÃ¼"""
    return HealthResponse(
        status="healthy",
        model_loaded=model is not None,
        index_size=len(content_data),
        llm_loaded=True  # HuggingFace Inference API kullanÄ±yoruz, her zaman hazÄ±r
    )


@app.post("/index", response_model=dict)
async def index_contents(request: IndexRequest):
    """Ä°Ã§erikleri indexle (embedding oluÅŸtur)"""
    global index, content_data
    
    if not request.contents:
        raise HTTPException(status_code=400, detail="Ä°Ã§erik listesi boÅŸ")
    
    load_model()
    
    # Ä°Ã§erikleri hazÄ±rla
    content_data = [item.dict() for item in request.contents]
    texts = [create_search_text(item) for item in content_data]
    
    print(f"ğŸ”„ {len(texts)} iÃ§erik iÃ§in embedding oluÅŸturuluyor...")
    
    # Embedding oluÅŸtur
    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)
    
    # Normalize et (cosine similarity iÃ§in)
    faiss.normalize_L2(embeddings)
    
    # FAISS index oluÅŸtur
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)  # Inner Product = Cosine Similarity (normalized iÃ§in)
    index.add(embeddings)
    
    # Disk'e kaydet
    save_index_to_disk()
    
    print(f"âœ… Index oluÅŸturuldu: {index.ntotal} iÃ§erik")
    
    return {
        "success": True,
        "indexed_count": len(content_data),
        "dimension": dimension
    }


@app.post("/search", response_model=SearchResponse)
async def semantic_search(request: SearchRequest):
    """Semantic search yap"""
    global index, content_data
    
    if index is None or len(content_data) == 0:
        raise HTTPException(status_code=400, detail="Index henÃ¼z oluÅŸturulmamÄ±ÅŸ. Ã–nce /index endpoint'ini Ã§aÄŸÄ±rÄ±n.")
    
    load_model()
    
    # Query embedding
    query_embedding = model.encode([request.query], convert_to_numpy=True)
    faiss.normalize_L2(query_embedding)
    
    # Arama yap
    k = min(request.limit * 2, len(content_data))  # Filtreleme iÃ§in fazla al
    scores, indices = index.search(query_embedding, k)
    
    results = []
    for score, idx in zip(scores[0], indices[0]):
        if idx == -1:
            continue
            
        item = content_data[idx]
        
        # TÃ¼r filtresi
        if request.tur and item.get('tur', '').lower() != request.tur.lower():
            continue
        
        results.append(SearchResult(
            id=item.get('id', idx),
            baslik=item.get('baslik', ''),
            tur=item.get('tur', ''),
            aciklama=item.get('aciklama', '')[:200] + '...' if len(item.get('aciklama', '')) > 200 else item.get('aciklama', ''),
            yil=item.get('yil'),
            posterUrl=item.get('posterUrl'),
            puan=item.get('puan'),
            score=float(score),
            neden=generate_reason(request.query, item, float(score))
        ))
        
        if len(results) >= request.limit:
            break
    
    return SearchResponse(
        results=results,
        query=request.query,
        total=len(results)
    )


@app.post("/embed")
async def get_embedding(text: str):
    """Tek bir metin iÃ§in embedding dÃ¶ndÃ¼r (debug iÃ§in)"""
    load_model()
    embedding = model.encode([text], convert_to_numpy=True)
    return {"embedding": embedding[0].tolist(), "dimension": len(embedding[0])}


@app.post("/generate", response_model=GenerateResponse)
async def generate_text(request: GenerateRequest):
    """LLM ile metin Ã¼ret"""
    pipe = load_llm()
    
    if pipe is None:
        raise HTTPException(status_code=503, detail="LLM henÃ¼z yÃ¼klenmedi, lÃ¼tfen bekleyin")
    
    try:
        # Phi-3 chat format
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.prompt})
        
        result = pipe(
            messages,
            max_new_tokens=request.max_tokens,
            temperature=request.temperature,
            do_sample=True,
            pad_token_id=llm_tokenizer.eos_token_id
        )
        
        generated_text = result[0]["generated_text"]
        
        return GenerateResponse(
            text=generated_text,
            tokens_used=len(llm_tokenizer.encode(generated_text))
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ãœretim hatasÄ±: {str(e)}")


@app.post("/recommend")
async def smart_recommend(request: RecommendRequest):
    """Semantic search + LLM ile akÄ±llÄ± Ã¶neri"""
    global index, content_data
    
    if index is None or len(content_data) == 0:
        raise HTTPException(status_code=400, detail="Index henÃ¼z oluÅŸturulmamÄ±ÅŸ")
    
    load_model()
    pipe = load_llm()
    
    # Ã–nce semantic search ile benzer iÃ§erikleri bul
    query_embedding = model.encode([request.query], convert_to_numpy=True)
    faiss.normalize_L2(query_embedding)
    
    k = min(request.limit * 3, len(content_data))
    scores, indices = index.search(query_embedding, k)
    
    candidates = []
    for score, idx in zip(scores[0], indices[0]):
        if idx == -1:
            continue
        item = content_data[idx]
        if request.tur and item.get('tur', '').lower() != request.tur.lower():
            continue
        candidates.append({
            **item,
            "score": float(score)
        })
        if len(candidates) >= request.limit:
            break
    
    # LLM ile aÃ§Ä±klama ekle (opsiyonel, LLM yoksa sadece search sonucu dÃ¶ner)
    if pipe and candidates:
        for item in candidates:
            item["neden"] = f"'{request.query}' aramanÄ±za benzer iÃ§erik"
    else:
        for item in candidates:
            item["neden"] = generate_reason(request.query, item, item["score"])
    
    return {
        "query": request.query,
        "results": candidates,
        "total": len(candidates)
    }


@app.post("/yearly-summary", response_model=YearlySummaryResponse)
async def generate_yearly_summary(request: YearlySummaryRequest):
    """YÄ±llÄ±k Ã¶zet iÃ§in Spotify Wrapped tarzÄ± anlatÄ± Ã¼ret - Groq API ile"""
    
    try:
        # Prompt oluÅŸtur
        prompt = f"""Sen bir medya asistanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n yÄ±llÄ±k izleme/okuma istatistiklerini Spotify Wrapped tarzÄ±nda, eÄŸlenceli ve samimi bir dille anlat. TÃ¼rkÃ§e yaz.

KullanÄ±cÄ±: {request.kullanici_adi}
YÄ±l: {request.yil}

Ä°statistikler:
- Toplam iÃ§erik: {request.toplam_icerik}
- Film: {request.film_sayisi}
- Dizi: {request.dizi_sayisi}
- Kitap: {request.kitap_sayisi}
- Toplam izleme sÃ¼resi: {request.toplam_dakika} dakika ({request.toplam_dakika // 60} saat)
- Okunan sayfa: {request.toplam_sayfa}
- En sevilen tÃ¼rler: {', '.join(request.en_cok_izlenen_turler[:3]) if request.en_cok_izlenen_turler else 'BelirtilmemiÅŸ'}
- En yÃ¼ksek puanlananlar: {', '.join(request.en_yuksek_puanlilar[:3]) if request.en_yuksek_puanlilar else 'BelirtilmemiÅŸ'}
- Ortalama puan: {request.ortalama_puan}

KÄ±sa (4-5 cÃ¼mle), eÄŸlenceli, samimi ve kiÅŸisel bir Ã¶zet yaz. 
- KullanÄ±cÄ±nÄ±n izleme alÄ±ÅŸkanlÄ±klarÄ±nÄ± analiz et
- Esprili ve sÄ±cak bir dil kullan
- Emoji kullan ğŸ¬ğŸ“ºğŸ“š
- KullanÄ±cÄ±yÄ± Ã¶zel hissettir"""

        messages = [
            {"role": "system", "content": "Sen eÄŸlenceli, samimi ve yaratÄ±cÄ± bir medya asistanÄ±sÄ±n. Spotify Wrapped tarzÄ±nda kiÅŸiselleÅŸtirilmiÅŸ Ã¶zetler yazÄ±yorsun."},
            {"role": "user", "content": prompt}
        ]
        
        # Groq API veya lokal LLM kullan
        narrative = await call_local_llm(messages, max_tokens=400)
        
        if not narrative:
            # API baÅŸarÄ±sÄ±z olursa fallback kullan
            return YearlySummaryResponse(
                title=f"ğŸ¬ {request.kullanici_adi}'Ä±n {request.yil} YÄ±lÄ±",
                narrative=generate_fallback_narrative(request)
            )
        
        # BaÅŸlÄ±k oluÅŸtur
        title = f"ğŸ¬ {request.kullanici_adi}'Ä±n {request.yil} MacerasÄ±"
        
        return YearlySummaryResponse(
            title=title,
            narrative=narrative
        )
    except Exception as e:
        print(f"LLM hatasÄ±: {e}")
        return YearlySummaryResponse(
            title=f"ğŸ¬ {request.kullanici_adi}'Ä±n {request.yil} YÄ±lÄ±",
            narrative=generate_fallback_narrative(request)
        )


def generate_fallback_narrative(request: YearlySummaryRequest) -> str:
    """LLM olmadan basit anlatÄ± oluÅŸtur"""
    saat = request.toplam_dakika // 60
    
    parts = [f"Bu yÄ±l tam {request.toplam_icerik} iÃ§erik keÅŸfettin! ğŸ‰"]
    
    if request.film_sayisi > request.dizi_sayisi:
        parts.append(f"{request.film_sayisi} film ile sinema tutkunu olduÄŸun belli.")
    elif request.dizi_sayisi > 0:
        parts.append(f"{request.dizi_sayisi} dizi bitirdin, maratoncusun!")
    
    if request.kitap_sayisi > 0:
        parts.append(f"{request.toplam_sayfa} sayfa okuyarak kitap kurtlarÄ±na katÄ±ldÄ±n. ğŸ“š")
    
    if saat > 100:
        parts.append(f"{saat} saatlik izleme sÃ¼resiyle gerÃ§ek bir iÃ§erik gurmesisin!")
    
    if request.en_cok_izlenen_turler:
        parts.append(f"En sevdiÄŸin tÃ¼r: {request.en_cok_izlenen_turler[0]}.")
    
    return " ".join(parts)


# Bilinen popÃ¼ler iÃ§erikler iÃ§in pattern matching
KNOWN_CONTENT_PATTERNS = [
    # Film patterns
    {
        "patterns": ["penÃ§e", "pence", "wolverine", "adamantium", "x-men", "xmen", "logan"],
        "title": "X-Men",
        "title_en": "X-Men",
        "tur": "film",
        "year": 2000,
        "explanation": "Wolverine karakteri ellerinden Ã§Ä±kan adamantium penÃ§eleriyle tanÄ±nÄ±r"
    },
    {
        "patterns": ["yeÅŸil dev", "kÄ±zÄ±nca yeÅŸil", "hulk", "bruce banner", "gamma", "Ã¶fkelenince"],
        "title": "Hulk",
        "title_en": "The Incredible Hulk",
        "tur": "film",
        "year": 2008,
        "explanation": "Bruce Banner Ã¶fkelendiÄŸinde yeÅŸil bir deve dÃ¶nÃ¼ÅŸÃ¼r"
    },
    {
        "patterns": ["Ã¶rÃ¼mcek", "spider", "peter parker", "aÄŸ atar", "duvar tÄ±rman"],
        "title": "Ã–rÃ¼mcek Adam",
        "title_en": "Spider-Man",
        "tur": "film",
        "year": 2002,
        "explanation": "Peter Parker Ã¶rÃ¼mcek tarafÄ±ndan Ä±sÄ±rÄ±larak sÃ¼per gÃ¼Ã§ler kazanÄ±r"
    },
    {
        "patterns": ["demir adam", "iron man", "tony stark", "zÄ±rh", "arc reactor"],
        "title": "Demir Adam",
        "title_en": "Iron Man",
        "tur": "film",
        "year": 2008,
        "explanation": "Tony Stark'Ä±n yarattÄ±ÄŸÄ± teknolojik zÄ±rh"
    },
    {
        "patterns": ["yarasa", "batman", "gotham", "bruce wayne", "kara ÅŸÃ¶valye"],
        "title": "Batman",
        "title_en": "The Dark Knight",
        "tur": "film",
        "year": 2008,
        "explanation": "Bruce Wayne gece yarasa kostÃ¼mÃ¼yle suÃ§la savaÅŸÄ±r"
    },
    {
        "patterns": ["joker", "palyaÃ§o", "neden bu kadar ciddi", "why so serious"],
        "title": "Kara ÅÃ¶valye",
        "title_en": "The Dark Knight",
        "tur": "film",
        "year": 2008,
        "explanation": "Joker'in 'Why so serious?' repliÄŸiyle Ã¼nlÃ¼ film"
    },
    {
        "patterns": ["thanos", "eldiven", "parmak ÅŸÄ±klat", "infinity", "yarÄ±sÄ± yok"],
        "title": "Avengers: Sonsuzluk SavaÅŸÄ±",
        "title_en": "Avengers: Infinity War",
        "tur": "film",
        "year": 2018,
        "explanation": "Thanos Sonsuzluk Eldiveni ile evrenin yarÄ±sÄ±nÄ± yok eder"
    },
    {
        "patterns": ["matrix", "kÄ±rmÄ±zÄ± hap", "neo", "morpheus", "gerÃ§eklik simÃ¼lasyon"],
        "title": "Matrix",
        "title_en": "The Matrix",
        "tur": "film",
        "year": 1999,
        "explanation": "Neo gerÃ§ekliÄŸin bir simÃ¼lasyon olduÄŸunu keÅŸfeder"
    },
    {
        "patterns": ["yÃ¼zÃ¼k", "frodo", "mordor", "gandalf", "hobbit", "sauron", "orta dÃ¼nya"],
        "title": "YÃ¼zÃ¼klerin Efendisi",
        "title_en": "The Lord of the Rings",
        "tur": "film",
        "year": 2001,
        "explanation": "Frodo Tek YÃ¼zÃ¼k'Ã¼ yok etmek iÃ§in Mordor'a yolculuk eder"
    },
    {
        "patterns": ["hogwarts", "harry potter", "bÃ¼yÃ¼cÃ¼", "voldemort", "asasÄ±", "quidditch"],
        "title": "Harry Potter",
        "title_en": "Harry Potter",
        "tur": "film",
        "year": 2001,
        "explanation": "BÃ¼yÃ¼cÃ¼ Ã§ocuk Harry Potter'Ä±n Hogwarts maceralarÄ±"
    },
    {
        "patterns": ["titanic", "gemi batÄ±yor", "buz daÄŸÄ±", "jack", "rose", "kalbim devam"],
        "title": "Titanik",
        "title_en": "Titanic",
        "tur": "film",
        "year": 1997,
        "explanation": "Jack ve Rose'un trajik aÅŸk hikayesi"
    },
    {
        "patterns": ["inception", "rÃ¼ya iÃ§inde rÃ¼ya", "totem", "rÃ¼yaya gir"],
        "title": "BaÅŸlangÄ±Ã§",
        "title_en": "Inception",
        "tur": "film",
        "year": 2010,
        "explanation": "RÃ¼yalarÄ±n iÃ§ine girip fikir Ã§alma/yerleÅŸtirme"
    },
    {
        "patterns": ["interstellar", "kara delik", "uzay zaman", "murph", "5. boyut"],
        "title": "YÄ±ldÄ±zlararasÄ±",
        "title_en": "Interstellar",
        "tur": "film",
        "year": 2014,
        "explanation": "Ä°nsanlÄ±ÄŸÄ± kurtarmak iÃ§in uzay yolculuÄŸu"
    },
    {
        "patterns": ["fight club", "dÃ¶vÃ¼ÅŸ kulÃ¼bÃ¼", "tyler durden", "ilk kural konuÅŸma"],
        "title": "DÃ¶vÃ¼ÅŸ KulÃ¼bÃ¼",
        "title_en": "Fight Club",
        "tur": "film",
        "year": 1999,
        "explanation": "Tyler Durden'Ä±n kurduÄŸu yeraltÄ± dÃ¶vÃ¼ÅŸ kulÃ¼bÃ¼"
    },
    {
        "patterns": ["forrest gump", "koÅŸ forrest", "Ã§ikolata kutusu", "hayat kutu"],
        "title": "Forrest Gump",
        "title_en": "Forrest Gump",
        "tur": "film",
        "year": 1994,
        "explanation": "Forrest Gump'Ä±n olaÄŸanÃ¼stÃ¼ hayat hikayesi"
    },
    # Dizi patterns
    {
        "patterns": ["breaking bad", "heisenberg", "meth", "walter white", "kimya Ã¶ÄŸretmen", "kimya ogretmen", "uyuÅŸturucu Ã¼ret", "uyusturucu uret", "blue meth"],
        "title": "Breaking Bad",
        "title_en": "Breaking Bad",
        "tur": "dizi",
        "year": 2008,
        "explanation": "Kimya Ã¶ÄŸretmeni uyuÅŸturucu imparatorluÄŸu kurar"
    },
    {
        "patterns": ["game of thrones", "taht oyunlarÄ±", "westeros", "ejderha", "kÄ±ÅŸ geliyor"],
        "title": "Taht OyunlarÄ±",
        "title_en": "Game of Thrones",
        "tur": "dizi",
        "year": 2011,
        "explanation": "Demir Taht iÃ§in krallÄ±klar savaÅŸÄ±"
    },
    {
        "patterns": ["stranger things", "demogorgon", "upside down", "eleven", "80ler"],
        "title": "Stranger Things",
        "title_en": "Stranger Things",
        "tur": "dizi",
        "year": 2016,
        "explanation": "Hawkins kasabasÄ±nÄ±n paranormal olaylarÄ±"
    },
    {
        "patterns": ["friends", "central perk", "ross rachel", "how you doin"],
        "title": "Friends",
        "title_en": "Friends",
        "tur": "dizi",
        "year": 1994,
        "explanation": "New York'ta 6 arkadaÅŸÄ±n hikayesi"
    },
    {
        "patterns": ["office", "michael scott", "dunder mifflin", "that's what she said"],
        "title": "Ofis",
        "title_en": "The Office",
        "tur": "dizi",
        "year": 2005,
        "explanation": "KaÄŸÄ±t ÅŸirketindeki ofis Ã§alÄ±ÅŸanlarÄ±nÄ±n hayatÄ±"
    },
    {
        "patterns": ["money heist", "la casa de papel", "professor", "bella ciao", "soygun"],
        "title": "La Casa de Papel",
        "title_en": "Money Heist",
        "tur": "dizi",
        "year": 2017,
        "explanation": "Darphane soygunu planÄ±"
    },
    # Kitap patterns
    {
        "patterns": ["1984", "bÃ¼yÃ¼k birader", "big brother", "orwell"],
        "title": "1984",
        "title_en": "1984",
        "tur": "kitap",
        "year": 1949,
        "explanation": "George Orwell'Ä±n distopik romanÄ±"
    },
    {
        "patterns": ["suÃ§ ve ceza", "raskolnikov", "dostoyevski", "cinayet vicdan"],
        "title": "SuÃ§ ve Ceza",
        "title_en": "Crime and Punishment",
        "tur": "kitap",
        "year": 1866,
        "explanation": "Raskolnikov'un cinayet ve vicdan azabÄ± hikayesi"
    },
]


def match_known_content(description: str) -> Optional[IdentifyResponse]:
    """Bilinen iÃ§erik kalÄ±plarÄ±nÄ± eÅŸleÅŸtir"""
    desc_lower = description.lower()
    
    for content in KNOWN_CONTENT_PATTERNS:
        for pattern in content["patterns"]:
            if pattern.lower() in desc_lower:
                return IdentifyResponse(
                    found=True,
                    title=content["title"],
                    title_en=content["title_en"],
                    tur=content["tur"],
                    year=content["year"],
                    explanation=content["explanation"],
                    confidence=0.95,
                    search_query=content["title_en"]
                )
    
    return None


@app.post("/identify", response_model=IdentifyResponse)
async def identify_content(request: IdentifyRequest):
    """
    KullanÄ±cÄ±nÄ±n tanÄ±mÄ±ndan veya sorusundan film/dizi/kitap adÄ± dÃ¶ndÃ¼r.
    - TanÄ±m: "ellerinden penÃ§e Ã§Ä±kan adam" -> "X-Men"
    - Soru: "en popÃ¼ler dizi" -> "Game of Thrones" gibi popÃ¼ler bir dizi Ã¶ner
    """
    
    # Ã–nce bilinen popÃ¼ler iÃ§erikler iÃ§in pattern matching dene
    known_content = match_known_content(request.description)
    if known_content:
        return known_content
    
    # HuggingFace Inference API ile LLM Ã§aÄŸÄ±r
    try:
        tur_hint = ""
        if request.tur:
            tur_map = {"film": "film", "dizi": "TV dizisi", "kitap": "kitap"}
            tur_hint = f"Bu bir {tur_map.get(request.tur, request.tur)} olmalÄ±."
        
        # Kimi-K2 iÃ§in OpenAI uyumlu format
        system_prompt = """Sen bir film, dizi ve kitap uzmanÄ±sÄ±n. 

GÃ–REV: KullanÄ±cÄ±nÄ±n isteÄŸine gÃ¶re GERÃ‡EK, VAR OLAN bir iÃ§erik adÄ± dÃ¶ndÃ¼r!

KURALLAR:
1. Her zaman GERÃ‡EK bir film/dizi/kitap adÄ± ver (Ã¶rn: "Breaking Bad", "Harry Potter", "1984")
2. ASLA genel ifadeler kullanma (Ã¶rn: "En PopÃ¼ler Dizi" YANLIÅ, "Game of Thrones" DOÄRU)
3. ASLA kategorik isimler verme (Ã¶rn: "Best-Selling Book" YANLIÅ, "Don KiÅŸot" DOÄRU)
4. KullanÄ±cÄ± "Ã¶neri" istiyorsa popÃ¼ler ve kaliteli bir iÃ§erik Ã¶ner

SADECE JSON formatÄ±nda cevap ver, baÅŸka hiÃ§bir ÅŸey yazma."""
        
        user_prompt = f"""KullanÄ±cÄ±nÄ±n metni: "{request.description}"
{tur_hint}

DOÄRU Ã–RNEKLER (GERÃ‡EK Ä°Ã‡ERÄ°K ADLARI):
- "en popÃ¼ler dizi" -> title: "Breaking Bad"
- "iyi bir film Ã¶ner" -> title: "Esaretin Bedeli", title_en: "The Shawshank Redemption"
- "kitap Ã¶ner" -> title: "SuÃ§ ve Ceza", title_en: "Crime and Punishment"
- "korku filmi" -> title: "Åeytan", title_en: "The Exorcist"
- "bilim kurgu dizisi" -> title: "Black Mirror"
- "romantik kitap" -> title: "AÅŸk", title_en: "Love"
- "komedi filmi" -> title: "Maskeli BeÅŸler"

YANLIÅ Ã–RNEKLER (BUNLARI YAPMA):
- title: "En PopÃ¼ler Dizi" âŒ
- title: "Best-Selling Book" âŒ
- title: "Ä°yi Film" âŒ

JSON formatÄ±nda cevap ver:
{{
    "found": true,
    "title": "GERÃ‡EK iÃ§erik adÄ± (TÃ¼rkÃ§e)",
    "title_en": "GERÃ‡EK iÃ§erik adÄ± (Ä°ngilizce)",
    "tur": "film/dizi/kitap",
    "year": yÄ±l veya null,
    "explanation": "KÄ±sa aÃ§Ä±klama",
    "confidence": 0.8
}}"""

        response_text = await call_hf_inference_api(user_prompt, max_tokens=250, system_prompt=system_prompt)
        
        if not response_text:
            # API Ã§alÄ±ÅŸmadÄ±, pattern matching sonucunu dÃ¶ndÃ¼r
            return IdentifyResponse(
                found=False,
                title="",
                title_en=None,
                tur=request.tur or "film",
                year=None,
                explanation="LLM API'ye ulaÅŸÄ±lamadÄ±",
                confidence=0.0,
                search_query=request.description
            )
        
        print(f"LLM yanÄ±tÄ±: {response_text}")
        
        # JSON parse et
        import re
        json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
        
        if json_match:
            parsed = json.loads(json_match.group())
            
            title = parsed.get("title", "")
            title_en = parsed.get("title_en", title)
            tur = parsed.get("tur", "film")
            confidence = float(parsed.get("confidence", 0.7))
            
            # Genel/sahte isim kontrolÃ¼ - bunlar gerÃ§ek iÃ§erik deÄŸil
            fake_patterns = [
                "en popÃ¼ler", "en iyi", "best", "most", "top", "popular",
                "Ã¶neri", "tavsiye", "recommend", "selling", "Ã§ok okunan",
                "iyi film", "iyi dizi", "iyi kitap", "good", "great",
                "excellent", "amazing", "awesome", "wonderful", "perfect",
                "bir film", "bir dizi", "bir kitap", "a book", "a movie",
                "an excellent", "a great", "a good", "the best"
            ]
            title_lower = title.lower() if title else ""
            title_en_lower = title_en.lower() if title_en else ""
            is_fake_title = (
                any(p in title_lower for p in fake_patterns) or 
                any(p in title_en_lower for p in fake_patterns) or 
                len(title) < 3 or
                title_lower in ["film", "dizi", "kitap", "book", "movie", "series", "show"]
            )
            
            # BaÅŸlÄ±k yoksa veya sahte isimse fallback Ã¶ner
            if not title or is_fake_title:
                import random
                # Tur'a gÃ¶re popÃ¼ler iÃ§erik listesinden rastgele seÃ§
                fallback_lists = {
                    "dizi": [
                        ("Breaking Bad", "Breaking Bad", "Efsanevi suÃ§ dizisi"),
                        ("Game of Thrones", "Game of Thrones", "Epik fantastik dizi"),
                        ("Stranger Things", "Stranger Things", "Nostaljik bilim kurgu"),
                        ("The Wire", "The Wire", "GerÃ§ekÃ§i suÃ§ dramasÄ±"),
                        ("Chernobyl", "Chernobyl", "Tarihi mini dizi"),
                    ],
                    "film": [
                        ("Esaretin Bedeli", "The Shawshank Redemption", "TÃ¼m zamanlarÄ±n en iyi filmi"),
                        ("Baba", "The Godfather", "Efsanevi mafya filmi"),
                        ("Kara ÅÃ¶valye", "The Dark Knight", "En iyi sÃ¼per kahraman filmi"),
                        ("YÄ±ldÄ±zlararasÄ±", "Interstellar", "Epik bilim kurgu"),
                        ("DÃ¶vÃ¼ÅŸ KulÃ¼bÃ¼", "Fight Club", "KÃ¼lt klasik"),
                    ],
                    "kitap": [
                        ("SuÃ§ ve Ceza", "Crime and Punishment", "Dostoyevski'nin baÅŸyapÄ±tÄ±"),
                        ("1984", "1984", "Orwell'Ä±n distopyasÄ±"),
                        ("YÃ¼zÃ¼klerin Efendisi", "The Lord of the Rings", "Epik fantastik"),
                        ("SavaÅŸ ve BarÄ±ÅŸ", "War and Peace", "Tolstoy'un ÅŸaheseri"),
                        ("KÃ¼Ã§Ã¼k Prens", "The Little Prince", "ZamansÄ±z klasik"),
                    ]
                }
                detected_tur = request.tur or tur or "film"
                fallback_list = fallback_lists.get(detected_tur, fallback_lists["film"])
                title, title_en, explanation = random.choice(fallback_list)
                tur = detected_tur
                parsed["explanation"] = explanation
            
            # Arama sorgusu oluÅŸtur
            search_query = title_en if title_en else title
            
            return IdentifyResponse(
                found=True,
                title=title,
                title_en=title_en,
                tur=tur,
                year=parsed.get("year"),
                explanation=parsed.get("explanation", ""),
                confidence=confidence,
                search_query=search_query
            )
        else:
            # JSON bulunamadÄ±
            return IdentifyResponse(
                found=False,
                title="",
                title_en=None,
                tur=request.tur or "film",
                year=None,
                explanation="TanÄ±mdan iÃ§erik belirlenemedi",
                confidence=0.0,
                search_query=request.description
            )
            
    except json.JSONDecodeError as e:
        print(f"JSON parse hatasÄ±: {e}")
        return IdentifyResponse(
            found=False,
            title="",
            title_en=None,
            tur=request.tur or "film",
            year=None,
            explanation="YanÄ±t iÅŸlenemedi",
            confidence=0.0,
            search_query=request.description
        )
    except Exception as e:
        print(f"Identify hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=f"Ä°Ã§erik tanÄ±mlama hatasÄ±: {str(e)}")


# ===== YENÄ°: Genel AI Chat Endpoint =====
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Genel sohbet endpoint'i - KullanÄ±cÄ±yla doÄŸal dilde konuÅŸma
    Film, dizi, kitap hakkÄ±nda her tÃ¼rlÃ¼ soruyu yanÄ±tlar
    """
    try:
        # Saga asistanÄ± system prompt'u
        system_prompt = """Sen Saga'nÄ±n AI asistanÄ±sÄ±n. Saga, kullanÄ±cÄ±larÄ±n film, dizi ve kitaplarÄ± takip ettiÄŸi bir platformdur.

GÃ¶revlerin:
1. Film, dizi ve kitaplar hakkÄ±nda bilgi vermek (Ã¶zet, oyuncular, yÃ¶netmenler, yazarlar, tÃ¼rler vs.)
2. Ä°Ã§erik Ã¶nerileri yapmak
3. KullanÄ±cÄ±nÄ±n sorularÄ±nÄ± yanÄ±tlamak
4. Platform hakkÄ±nda yardÄ±m etmek

Kurallar:
- TÃ¼rkÃ§e yanÄ±t ver
- KÄ±sa ve Ã¶z ol, gereksiz uzatma
- Spoiler vermekten kaÃ§Ä±n (kullanÄ±cÄ± aÃ§Ä±kÃ§a istemezse)
- Emin olmadÄ±ÄŸÄ±n bilgileri tahmin olarak belirt
- Samimi ve yardÄ±msever ol"""

        # MesajlarÄ± OpenAI formatÄ±na Ã§evir
        messages = [{"role": "system", "content": system_prompt}]
        
        # Kontekst varsa ekle
        if request.context:
            messages.append({"role": "system", "content": f"KullanÄ±cÄ± ÅŸu anda ÅŸu sayfada: {request.context}"})
        
        # Sohbet geÃ§miÅŸini ekle
        for msg in request.messages:
            messages.append({"role": msg.role, "content": msg.content})
        
        # HuggingFace Router API ile Ã§aÄŸÄ±r
        response_text = await call_hf_router_api(messages, request.max_tokens)
        
        if not response_text:
            return ChatResponse(
                message="AI yanÄ±t veremedi. LÃ¼tfen tekrar deneyin.",
                suggestions=None
            )
        
        # Takip sorularÄ± Ã¶ner
        suggestions = None
        if len(request.messages) <= 2:  # Ä°lk birkaÃ§ mesajda Ã¶neri ver
            suggestions = [
                "Bu iÃ§eriÄŸe benzer baÅŸka Ã¶nerilerin var mÄ±?",
                "OyuncularÄ±/yazarÄ± hakkÄ±nda bilgi verir misin?",
                "Bu iÃ§eriÄŸin puanÄ± nasÄ±l?"
            ]
        
        return ChatResponse(
            message=response_text,
            suggestions=suggestions
        )
        
    except Exception as e:
        print(f"Chat hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=f"Sohbet hatasÄ±: {str(e)}")


# ===== YENÄ°: Ä°Ã§erik HakkÄ±nda Soru-Cevap =====
@app.post("/content-question", response_model=ContentQuestionResponse)
async def content_question(request: ContentQuestionRequest):
    """
    Belirli bir iÃ§erik hakkÄ±nda soru yanÄ±tla
    Ã–rnek: "Inception filminin konusu ne?" veya "Bu kitabÄ±n yazarÄ± kim?"
    """
    try:
        system_prompt = f"""Sen bir {request.content_type} uzmanÄ±sÄ±n. KullanÄ±cÄ± "{request.content_title}" hakkÄ±nda soru soruyor.

Ä°Ã§erik bilgisi:
- BaÅŸlÄ±k: {request.content_title}
- TÃ¼r: {request.content_type}
{f'- AÃ§Ä±klama: {request.content_description}' if request.content_description else ''}

Kurallar:
- TÃ¼rkÃ§e yanÄ±t ver
- Spoiler vermekten kaÃ§Ä±n (aÃ§Ä±kÃ§a istenmezse)
- KÄ±sa ve bilgilendirici ol
- Emin olmadÄ±ÄŸÄ±n bilgileri belirt"""

        user_prompt = request.question
        
        # HuggingFace Router API ile Ã§aÄŸÄ±r
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        answer = await call_hf_router_api(messages, 400)
        
        if not answer:
            return ContentQuestionResponse(
                answer="AI yanÄ±t veremedi. LÃ¼tfen tekrar deneyin.",
                related_questions=None
            )
        
        # Ä°lgili sorular Ã¶ner
        related_questions = [
            f"{request.content_title} ile benzer iÃ§erikler neler?",
            f"Bu {request.content_type}Ä±n puanÄ± kaÃ§?",
            f"KÄ±saca Ã¶zet verir misin?"
        ]
        
        return ContentQuestionResponse(
            answer=answer,
            related_questions=related_questions
        )
        
    except Exception as e:
        print(f"Content question hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=f"Soru yanÄ±tlama hatasÄ±: {str(e)}")


# ===== YENÄ°: Site AsistanÄ± =====
@app.post("/assistant", response_model=AssistantResponse)
async def assistant(request: AssistantRequest):
    """
    Site genelinde akÄ±llÄ± asistan
    Navigasyon, arama, Ã¶neri ve bilgi saÄŸlar
    """
    try:
        query_lower = request.query.lower()
        
        # Ä°Ã§erik tanÄ±mlama isteÄŸi mi kontrol et
        # "bir film vardÄ±", "ÅŸu dizi", "hangi kitap", "bulmaya Ã§alÄ±ÅŸÄ±yorum" gibi ifadeler
        identify_keywords = [
            "bir film", "ÅŸu film", "hangi film", "o film",
            "bir dizi", "ÅŸu dizi", "hangi dizi", "o dizi", 
            "bir kitap", "ÅŸu kitap", "hangi kitap", "o kitap",
            "bulmaya Ã§alÄ±ÅŸÄ±yorum", "hatÄ±rlamaya Ã§alÄ±ÅŸÄ±yorum", "adÄ±nÄ± unuttum",
            "vardÄ±", "izlemiÅŸtim", "okumuÅŸtum", "gÃ¶rdÃ¼m",
            "ne filmdi", "ne dizisi", "ne kitabÄ±", "hangi", "hangisi"
        ]
        
        is_identify_request = any(kw in query_lower for kw in identify_keywords)
        
        if is_identify_request:
            # Ä°Ã§erik tanÄ±mlama moduna geÃ§
            identify_prompt = f"""KullanÄ±cÄ± bir film, dizi veya kitap tanÄ±mlamaya Ã§alÄ±ÅŸÄ±yor. 
TanÄ±mdan iÃ§eriÄŸin adÄ±nÄ± bul.

TanÄ±m: {request.query}

Sadece JSON formatÄ±nda yanÄ±t ver:
{{
    "found": true/false,
    "title": "Ä°Ã§eriÄŸin TÃ¼rkÃ§e adÄ±",
    "title_en": "Ä°Ã§eriÄŸin Ä°ngilizce adÄ± (varsa)",
    "content_type": "film/dizi/kitap",
    "message": "KullanÄ±cÄ±ya gÃ¶sterilecek samimi mesaj"
}}"""
            
            messages = [
                {"role": "system", "content": "Sen bir film, dizi ve kitap uzmanÄ±sÄ±n. TanÄ±mlardan iÃ§erikleri bul."},
                {"role": "user", "content": identify_prompt}
            ]
            
            response_text = await call_hf_router_api(messages, 250)
            
            if response_text:
                # JSON parse et
                json_str = extract_json(response_text)
                if json_str:
                    try:
                        parsed = json.loads(json_str)
                        if parsed.get("found") and parsed.get("title"):
                            title = parsed.get("title")
                            title_en = parsed.get("title_en")
                            content_type = parsed.get("content_type", "film")
                            
                            # Arama query'si oluÅŸtur
                            search_query = title_en if title_en else title
                            
                            message = parsed.get("message", f'"{title}" buldum! Aramaya yÃ¶nlendiriyorum...')
                            
                            return AssistantResponse(
                                message=message,
                                action="search",
                                action_data={"query": search_query, "title": title, "type": content_type},
                                suggestions=[f"{title} hakkÄ±nda bilgi ver", "Benzer iÃ§erikler Ã¶ner"]
                            )
                    except json.JSONDecodeError:
                        pass
            
            # BulunamadÄ±ysa
            return AssistantResponse(
                message="TanÄ±mladÄ±ÄŸÄ±n iÃ§eriÄŸi bulamadÄ±m. Biraz daha detay verebilir misin?",
                action=None,
                action_data=None,
                suggestions=["Daha fazla detay ekle", "BaÅŸka bir ÅŸekilde tanÄ±mla"]
            )
        
        # Normal asistan modu
        system_prompt = """Sen Saga platformunun yardÄ±mcÄ± asistanÄ±sÄ±n. KullanÄ±cÄ±lara kÄ±sa ve net yardÄ±m et.

Platform Ã¶zellikleri:
- KÃ¼tÃ¼phane (/kutuphane): Ä°zlenen film/dizi ve okunan kitaplar. Bir iÃ§eriÄŸi kÃ¼tÃ¼phaneye eklemek iÃ§in iÃ§erik sayfasÄ±ndaki "KÃ¼tÃ¼phaneye Ekle" butonuna tÄ±klanÄ±r.
- Listeler (/listeler): Ã–zel koleksiyonlar oluÅŸturma
- KeÅŸfet (/kesfet): Yeni iÃ§erik arama ve keÅŸfetme
- Profil (/profil): KullanÄ±cÄ± istatistikleri
- Aktivite (/aktivite): ArkadaÅŸlarÄ±n aktiviteleri

KURALLAR:
1. Sadece bilgi sorusu ise action=null olsun, sadece message ile yanÄ±tla
2. KullanÄ±cÄ± sayfaya gitmek istiyorsa action="navigate", action_data={"url": "/sayfa"}
3. KullanÄ±cÄ± arama yapmak istiyorsa action="search", action_data={"query": "arama terimi"}
4. KullanÄ±cÄ± Ã¶neri istiyorsa action="recommend"
5. message iÃ§ine JSON yazma, dÃ¼z metin yaz
6. Ã–nceki sohbeti dikkate al ve baÄŸlamÄ± koru

YanÄ±t formatÄ± (SADEce bu formatta):
{"message": "KÄ±sa yanÄ±t", "action": null, "action_data": null, "suggestions": ["Ã–neri"]}"""

        user_context_str = ""
        if request.user_context:
            user_context_str = f"\nKullanÄ±cÄ±: {request.user_context.get('username', 'misafir')}"
        
        page_context = ""
        if request.current_page:
            page_context = f" (Sayfa: {request.current_page})"
        
        user_prompt = f"Soru: {request.query}{page_context}{user_context_str}"

        # Mesaj listesi oluÅŸtur
        messages = [{"role": "system", "content": system_prompt}]
        
        # Sohbet geÃ§miÅŸini ekle (varsa)
        if request.chat_history:
            for msg in request.chat_history[-6:]:  # Son 6 mesaj (3 tur)
                messages.append({"role": msg.role, "content": msg.content})
        
        # Son kullanÄ±cÄ± mesajÄ±nÄ± ekle
        messages.append({"role": "user", "content": user_prompt})
        
        # HuggingFace Router API ile Ã§aÄŸÄ±r
        response_text = await call_hf_router_api(messages, 400)
        
        if not response_text:
            return AssistantResponse(
                message="AI asistan yanÄ±t veremedi. LÃ¼tfen tekrar deneyin.",
                action=None,
                action_data=None,
                suggestions=["KeÅŸfet sayfasÄ±na git", "KÃ¼tÃ¼phaneme bak"]
            )
        
        # Yeni parse fonksiyonunu kullan (iÃ§ iÃ§e JSON desteÄŸi)
        parsed = parse_assistant_response(response_text)
        
        return AssistantResponse(
            message=parsed.get("message", response_text),
            action=parsed.get("action"),
            action_data=parsed.get("action_data"),
            suggestions=parsed.get("suggestions")
        )
        
    except Exception as e:
        print(f"Assistant hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=f"Asistan hatasÄ±: {str(e)}")


# ===== YENÄ°: Ã–zet Ä°ste =====
@app.post("/summarize")
async def summarize_content(content_title: str, content_type: str, spoiler_free: bool = True):
    """
    Bir iÃ§eriÄŸin Ã¶zetini al
    """
    try:
        spoiler_note = "SPOILER VERME!" if spoiler_free else "Spoiler verebilirsin."
        
        system_prompt = f"""Sen bir {content_type} uzmanÄ±sÄ±n. "{content_title}" iÃ§in kÄ±sa bir Ã¶zet yaz.
{spoiler_note}
TÃ¼rkÃ§e yaz. 2-3 paragraf yeterli."""

        # HuggingFace Router API ile Ã§aÄŸÄ±r
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"{content_title} hakkÄ±nda Ã¶zet ver."}
        ]
        summary = await call_hf_router_api(messages, 500)
        
        if not summary:
            return {"summary": "AI servisi ÅŸu anda kullanÄ±lamÄ±yor.", "spoiler_free": spoiler_free}
        
        return {
            "title": content_title,
            "type": content_type,
            "summary": summary,
            "spoiler_free": spoiler_free
        }
        
    except Exception as e:
        print(f"Summarize hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=f"Ã–zet hatasÄ±: {str(e)}")


# Gradio interface (HuggingFace Spaces iÃ§in)
def create_gradio_interface():
    """Gradio arayÃ¼zÃ¼ oluÅŸtur (opsiyonel)"""
    try:
        import gradio as gr
        
        def search_ui(query: str, tur: str, limit: int):
            if index is None:
                return "Index yok. Ã–nce iÃ§erikleri yÃ¼kleyin."
            
            tur_filter = tur if tur != "Hepsi" else None
            
            query_embedding = model.encode([query], convert_to_numpy=True)
            faiss.normalize_L2(query_embedding)
            
            scores, indices = index.search(query_embedding, limit)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx == -1:
                    continue
                item = content_data[idx]
                if tur_filter and item.get('tur', '').lower() != tur_filter.lower():
                    continue
                results.append(f"**{item.get('baslik')}** ({item.get('tur')}) - Skor: {score:.2f}\n{item.get('aciklama', '')[:100]}...")
            
            return "\n\n---\n\n".join(results) if results else "SonuÃ§ bulunamadÄ±"
        
        interface = gr.Interface(
            fn=search_ui,
            inputs=[
                gr.Textbox(label="Arama", placeholder="Film/kitap anlat veya ara..."),
                gr.Dropdown(["Hepsi", "film", "dizi", "kitap"], label="TÃ¼r", value="Hepsi"),
                gr.Slider(1, 10, value=5, step=1, label="SonuÃ§ SayÄ±sÄ±")
            ],
            outputs=gr.Markdown(label="SonuÃ§lar"),
            title="ğŸ¬ Saga AI Search",
            description="Film, dizi ve kitap iÃ§in semantic arama"
        )
        
        return interface
    except ImportError:
        return None


# HuggingFace Spaces iÃ§in
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=7860)
